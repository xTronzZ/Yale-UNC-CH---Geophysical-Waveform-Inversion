{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TRAIN = True # bfloat16 or float32 recommended\n",
    "RUN_VALID = False\n",
    "RUN_TEST  = False\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n",
    "    raise RuntimeError(\"Requires >= 2 GPUs with CUDA enabled.\")\n",
    "\n",
    "try: \n",
    "    import monai\n",
    "except: \n",
    "    !pip install --no-deps monai -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting _cfg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile _cfg.py\n",
    "\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "\n",
    "cfg= SimpleNamespace()\n",
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg.local_rank = 0\n",
    "cfg.seed = 777\n",
    "cfg.subsample = None\n",
    "\n",
    "cfg.backbone = \"caformer_b36.sail_in22k_ft_in1k\"\n",
    "# cfg.backbone = \"caformer_m36.sail_in22k_ft_in1k_384\"\n",
    "\n",
    "cfg.ema = True\n",
    "cfg.ema_decay = 0.99\n",
    "\n",
    "cfg.epochs = 200\n",
    "cfg.batch_size = 16\n",
    "cfg.batch_size_val = 16\n",
    "\n",
    "cfg.early_stopping = {\"patience\": 10, \"streak\": 0}\n",
    "cfg.logging_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Almost the same as the starter notebook. \n",
    "\n",
    "The input dataset changes to Egor's [openfwi_float16_1](https://www.kaggle.com/datasets/egortrushin/open-wfi-1) and [openfwi_float16_2](https://www.kaggle.com/datasets/egortrushin/open-wfi-2) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting _dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile _dataset.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        cfg,\n",
    "        mode = \"train\", \n",
    "        transform_data=None,\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.data, self.labels, self.records = self.load_metadata()\n",
    "\n",
    "    def load_metadata(self, ):\n",
    "\n",
    "        # Select rows\n",
    "        df= pd.read_csv(\"Data/datesets/brendanartley/openfwi-preprocessed-72x72/versions/7/folds.csv\")\n",
    "        \n",
    "        if self.cfg.subsample is not None:\n",
    "            df= df.groupby([\"dataset\", \"fold\"]).head(self.cfg.subsample)\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            df= df[df[\"fold\"] != 0]\n",
    "        else:\n",
    "            df= df[df[\"fold\"] == 0]\n",
    "\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        records = []\n",
    "        mmap_mode = \"r\"\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), disable=self.cfg.local_rank != 0):\n",
    "            row= row.to_dict()\n",
    "\n",
    "            # Hacky way to get exact file name\n",
    "            p1 = os.path.join(\"Data/datesets/egortrushin/open-wfi-1/versions/1/openfwi_float16_1\", row[\"data_fpath\"])\n",
    "            p2 = os.path.join(\"Data/datesets/egortrushin/open-wfi-1/versions/1/openfwi_float16_1\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n",
    "            p3 = os.path.join(\"Data/datesets/egortrushin/open-wfi-2/versions/1/openfwi_float16_2\", row[\"data_fpath\"])\n",
    "            p4 = os.path.join(\"Data/datesets/egortrushin/open-wfi-2/versions/1/openfwi_float16_2\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n",
    "            farr= glob.glob(p1) + glob.glob(p2) + glob.glob(p3) + glob.glob(p4)\n",
    "        \n",
    "            # Map to lbl fpath\n",
    "            farr= farr[0]\n",
    "            flbl= farr.replace('seis', 'vel').replace('data', 'model')\n",
    "            \n",
    "            # Load\n",
    "            arr= np.load(farr, mmap_mode=mmap_mode)\n",
    "            lbl= np.load(flbl, mmap_mode=mmap_mode)\n",
    "\n",
    "            # Append\n",
    "            data.append(arr)\n",
    "            labels.append(lbl)\n",
    "            records.append(row[\"dataset\"])\n",
    "\n",
    "        return data, labels, records\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_idx= idx // 500\n",
    "        col_idx= idx % 500\n",
    "\n",
    "        d= self.records[row_idx]\n",
    "        x= self.data[row_idx][col_idx, ...]\n",
    "        y= self.labels[row_idx][col_idx, ...]\n",
    "\n",
    "        # Augs \n",
    "        if self.mode == \"train\":\n",
    "            \n",
    "            # Temporal flip\n",
    "            if np.random.random() < 0.5:\n",
    "                x= x[::-1, :, ::-1]\n",
    "                y= y[..., ::-1]\n",
    "\n",
    "\n",
    "        x= x.copy()\n",
    "        y= y.copy()\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.records) * 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting _model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile _model.py\n",
    "\n",
    "from copy import deepcopy\n",
    "from types import MethodType\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "from timm.models.convnext import ConvNeXtBlock\n",
    "\n",
    "from monai.networks.blocks import UpSample, SubpixelUpsample\n",
    "\n",
    "####################\n",
    "## EMA + Ensemble ##\n",
    "####################\n",
    "\n",
    "class ModelEMA(nn.Module):\n",
    "    def __init__(self, model, decay=0.99, device=None):\n",
    "        super().__init__()\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n",
    "        \n",
    "\n",
    "#############\n",
    "## Decoder ##\n",
    "#############\n",
    "\n",
    "class ConvBnAct2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding: int = 0,\n",
    "        stride: int = 1,\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        act_layer: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            bias=False,\n",
    "        )\n",
    "        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n",
    "        self.act= act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SCSEModule2d(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, 1), \n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == \"scse\":\n",
    "            self.attention = SCSEModule2d(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "class DecoderBlock2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "        scale_factor: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsample block\n",
    "        if upsample_mode == \"pixelshuffle\":\n",
    "            self.upsample= SubpixelUpsample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = UpSample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                out_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "                mode= upsample_mode,\n",
    "            )\n",
    "\n",
    "        if intermediate_conv:\n",
    "            k= 3\n",
    "            c= skip_channels if skip_channels != 0 else in_channels\n",
    "            self.intermediate_conv = nn.Sequential(\n",
    "                ConvBnAct2d(c, c, k, k//2),\n",
    "                ConvBnAct2d(c, c, k, k//2),\n",
    "                )\n",
    "        else:\n",
    "            self.intermediate_conv= None\n",
    "\n",
    "        self.attention1 = Attention2d(\n",
    "            name= attention_type, \n",
    "            in_channels= in_channels + skip_channels,\n",
    "            )\n",
    "\n",
    "        self.conv1 = ConvBnAct2d(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "        )\n",
    "\n",
    "        self.conv2 = ConvBnAct2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "        )\n",
    "        self.attention2 = Attention2d(\n",
    "            name= attention_type, \n",
    "            in_channels= out_channels,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        if self.intermediate_conv is not None:\n",
    "            if skip is not None:\n",
    "                skip = self.intermediate_conv(skip)\n",
    "            else:\n",
    "                x = self.intermediate_conv(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            # print(x.shape, skip.shape)\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder2d_backbone1(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder.\n",
    "    Source: https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: tuple[int],\n",
    "        skip_channels: tuple[int] = None,\n",
    "        decoder_channels: tuple = (256, 128, 64, 32),\n",
    "        scale_factors: tuple = (2,2,2,2),\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = \"scse\",\n",
    "        intermediate_conv: bool = True,\n",
    "        upsample_mode: str = \"pixelshuffle\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if len(encoder_channels) == 4:\n",
    "            decoder_channels= decoder_channels[1:]\n",
    "        self.decoder_channels= decoder_channels\n",
    "        \n",
    "        if skip_channels is None:\n",
    "            skip_channels= list(encoder_channels[1:]) + [0]\n",
    "\n",
    "        # Build decoder blocks\n",
    "        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n",
    "            # print(i, ic, sc, dc)\n",
    "            self.blocks.append(\n",
    "                DecoderBlock2d(\n",
    "                    ic, sc, dc, \n",
    "                    norm_layer= norm_layer,\n",
    "                    attention_type= attention_type,\n",
    "                    intermediate_conv= intermediate_conv,\n",
    "                    upsample_mode= upsample_mode,\n",
    "                    scale_factor= scale_factors[i],\n",
    "                    )\n",
    "            )\n",
    "\n",
    "    def forward(self, feats: list[torch.Tensor]):\n",
    "        res= [feats[0]]\n",
    "        feats= feats[1:]\n",
    "\n",
    "        # Decoder blocks\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip= feats[i] if i < len(feats) else None\n",
    "            res.append(\n",
    "                b(res[-1], skip=skip),\n",
    "                )\n",
    "            \n",
    "        return res\n",
    "\n",
    "class UnetDecoder2d_backbone2(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder.\n",
    "    Source: https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: tuple[int],\n",
    "        skip_channels: tuple[int] = None,\n",
    "        decoder_channels: tuple = (256, 128, 64, 32),\n",
    "        scale_factors: tuple = (2,2,2,2),\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if len(encoder_channels) == 4:\n",
    "            decoder_channels= decoder_channels[1:]\n",
    "        self.decoder_channels= decoder_channels\n",
    "        \n",
    "        if skip_channels is None:\n",
    "            skip_channels= list(encoder_channels[1:]) + [0]\n",
    "\n",
    "        # Build decoder blocks\n",
    "        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n",
    "            # print(i, ic, sc, dc)\n",
    "            self.blocks.append(\n",
    "                DecoderBlock2d(\n",
    "                    ic, sc, dc, \n",
    "                    norm_layer= norm_layer,\n",
    "                    attention_type= attention_type,\n",
    "                    intermediate_conv= intermediate_conv,\n",
    "                    upsample_mode= upsample_mode,\n",
    "                    scale_factor= scale_factors[i],\n",
    "                    )\n",
    "            )\n",
    "\n",
    "    def forward(self, feats: list[torch.Tensor]):\n",
    "        res= [feats[0]]\n",
    "        feats= feats[1:]\n",
    "\n",
    "        # Decoder blocks\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip= feats[i] if i < len(feats) else None\n",
    "            res.append(\n",
    "                b(res[-1], skip=skip),\n",
    "                )\n",
    "            \n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "class SegmentationHead2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        scale_factor: tuple[int] = (2,2),\n",
    "        kernel_size: int = 3,\n",
    "        mode: str = \"nontrainable\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size= kernel_size,\n",
    "            padding= kernel_size//2\n",
    "        )\n",
    "        self.upsample = UpSample(\n",
    "            spatial_dims= 2,\n",
    "            in_channels= out_channels,\n",
    "            out_channels= out_channels,\n",
    "            scale_factor= scale_factor,\n",
    "            mode= mode,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _convnext_block_forward(self, x):\n",
    "    shortcut = x\n",
    "    x = self.conv_dw(x)\n",
    "\n",
    "    if self.use_conv_mlp:\n",
    "        x = self.norm(x)\n",
    "        x = self.mlp(x)\n",
    "    else:\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "        x = self.mlp(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.contiguous()\n",
    "\n",
    "    if self.gamma is not None:\n",
    "        x = x * self.gamma.reshape(1, -1, 1, 1)\n",
    "\n",
    "    x = self.drop_path(x) + self.shortcut(shortcut)\n",
    "    return x\n",
    "\n",
    "\n",
    "#############\n",
    "## Encoder ##\n",
    "#############\n",
    "\n",
    "class Net_backbone1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.backbone= timm.create_model(\n",
    "            backbone,\n",
    "            in_chans= 5,\n",
    "            pretrained= pretrained,\n",
    "            features_only= True,\n",
    "            drop_path_rate=0.0,\n",
    "            )\n",
    "        ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder= UnetDecoder2d_backbone1(\n",
    "            encoder_channels= ecs,\n",
    "        )\n",
    "\n",
    "        self.seg_head= SegmentationHead2d(\n",
    "            in_channels= self.decoder.decoder_channels[-1],\n",
    "            out_channels= 1,\n",
    "            scale_factor= 1,\n",
    "        )\n",
    "        \n",
    "        self._update_stem(backbone)\n",
    "\n",
    "    def _update_stem(self, backbone):\n",
    "        m = self.backbone\n",
    "\n",
    "        m.stem.conv.stride=(4,1)\n",
    "        m.stem.conv.padding=(0,4)\n",
    "        m.stages_0.downsample = nn.AvgPool2d(kernel_size=(4,1), stride=(4,1))\n",
    "        m.stem= nn.Sequential(\n",
    "            nn.ReflectionPad2d((0,0,78,78)),\n",
    "            m.stem,\n",
    "        )\n",
    "\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def proc_flip(self, x_in):\n",
    "        x_in= torch.flip(x_in, dims=[-3, -1])\n",
    "        x= self.backbone(x_in)\n",
    "        x= x[::-1]\n",
    "\n",
    "        # Decoder\n",
    "        x= self.decoder(x)\n",
    "        x_seg= self.seg_head(x[-1])\n",
    "        x_seg= x_seg[..., 1:-1, 1:-1]\n",
    "        x_seg= torch.flip(x_seg, dims=[-1])\n",
    "        x_seg= x_seg * 1500 + 3000\n",
    "        return x_seg\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x= batch\n",
    "\n",
    "        # Encoder\n",
    "        x_in = x\n",
    "        x= self.backbone(x)\n",
    "        # print([_.shape for _ in x])\n",
    "        x= x[::-1]\n",
    "\n",
    "        # Decoder\n",
    "        x= self.decoder(x)\n",
    "        # print([_.shape for _ in x])\n",
    "        x_seg= self.seg_head(x[-1])\n",
    "        x_seg= x_seg[..., 1:-1, 1:-1]\n",
    "        x_seg= x_seg * 1500 + 3000\n",
    "    \n",
    "        if self.training:\n",
    "            return x_seg\n",
    "        else:\n",
    "            p1 = self.proc_flip(x_in)\n",
    "            x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n",
    "            return x_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "\n",
    "Same as starter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting _utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile _utils.py\n",
    "\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Same as starter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting _train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile _train.py\n",
    "\n",
    "import os\n",
    "import time \n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from _cfg import cfg\n",
    "from _dataset import CustomDataset\n",
    "from _model import ModelEMA, Net_backbone1\n",
    "from _utils import format_time\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import logging\n",
    "\n",
    "import math\n",
    "\n",
    "import transforms as T\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"train_log.txt\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def set_seed(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    torch.cuda.set_device(rank)\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    return\n",
    "\n",
    "def cleanup():\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()\n",
    "    return\n",
    "\n",
    "def main(cfg):\n",
    "\n",
    "    # ========== Datasets / Dataloaders ==========\n",
    "\n",
    "    transform_data = Compose([\n",
    "        T.LogTransform(k=1),\n",
    "        T.MinMaxNormalize(T.log_transform(-61, k=1), T.log_transform(120, k=1))\n",
    "    ])\n",
    "    \n",
    "    if cfg.local_rank == 0:\n",
    "        logging.info(\"=\"*25)\n",
    "        logging.info(\"Loading data..\")\n",
    "\n",
    "    train_ds = CustomDataset(cfg=cfg, mode=\"train\")\n",
    "    \n",
    "    sampler= DistributedSampler(\n",
    "        train_ds, \n",
    "        num_replicas=cfg.world_size, \n",
    "        rank=cfg.local_rank,\n",
    "    )\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds, \n",
    "        sampler= sampler,\n",
    "        batch_size= cfg.batch_size, \n",
    "        num_workers= 4,\n",
    "    )\n",
    "    \n",
    "    valid_ds = CustomDataset(cfg=cfg, mode=\"valid\")\n",
    "    \n",
    "    sampler= DistributedSampler(\n",
    "        valid_ds, \n",
    "        num_replicas=cfg.world_size, \n",
    "        rank=cfg.local_rank,\n",
    "    )\n",
    "    valid_dl = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        sampler= sampler,\n",
    "        batch_size= cfg.batch_size_val, \n",
    "        num_workers= 4,\n",
    "    )\n",
    "\n",
    "    # ========== Model / Optim ==========\n",
    "    model = Net_backbone1(backbone=cfg.backbone)\n",
    "\n",
    "    # state_dict= torch.load(\"best_model_777.pt\", map_location=cfg.device, weights_only=True)\n",
    "    # state_dict= {k.removeprefix(\"_orig_mod.\"):v for k,v in state_dict.items()} # Remove torch.compile() prefix\n",
    "    \n",
    "    # model.load_state_dict(state_dict)\n",
    "    \n",
    "    model= model.to(cfg.local_rank)\n",
    "\n",
    "    # 用 torch.compile 加速\n",
    "    model = torch.compile(model, mode=\"max-autotune\")\n",
    "    \n",
    "    if cfg.ema:\n",
    "        if cfg.local_rank == 0:\n",
    "            print(\"Initializing EMA model..\")\n",
    "        ema_model = ModelEMA(\n",
    "            model, \n",
    "            decay=cfg.ema_decay, \n",
    "            device=cfg.local_rank,\n",
    "        )\n",
    "    else:\n",
    "        ema_model = None\n",
    "    model= DistributedDataParallel(\n",
    "        model, \n",
    "        device_ids=[cfg.local_rank], \n",
    "        )\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=True)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, min_lr=1e-8)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "\n",
    "    # ========== Training ==========\n",
    "    if cfg.local_rank == 0:\n",
    "        logging.info(\"=\"*25)\n",
    "        logging.info(\"Give me warp {}, Mr. Sulu.\".format(cfg.world_size))\n",
    "        logging.info(\"=\"*25)\n",
    "    \n",
    "    best_loss= 1_000_000\n",
    "    val_loss= 1_000_000\n",
    "\n",
    "    for epoch in range(0, cfg.epochs+1):\n",
    "        if epoch != 0:\n",
    "            tstart= time.time()\n",
    "            train_dl.sampler.set_epoch(epoch)\n",
    "    \n",
    "            # Train loop\n",
    "            model.train()\n",
    "            total_loss = []\n",
    "            for i, (x, y) in enumerate(train_dl):\n",
    "                x = x.to(cfg.local_rank)\n",
    "                y = y.to(cfg.local_rank)\n",
    "        \n",
    "                with autocast(device_type=cfg.device.type, dtype=torch.bfloat16):\n",
    "                    logits = model(x)\n",
    "                    \n",
    "                loss = criterion(logits, y)\n",
    "        \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "        \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "        \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                total_loss.append(loss.item())\n",
    "                \n",
    "                if ema_model is not None:\n",
    "                    ema_model.update(model)\n",
    "                    \n",
    "                if cfg.local_rank == 0 and (len(total_loss) >= cfg.logging_steps or i == 0):\n",
    "                    train_loss = np.mean(total_loss)\n",
    "                    total_loss = []\n",
    "                    logging.info(\"Epoch {}:     Train MAE: {:.2f}     Val MAE: {:.2f}     Time: {}     Step: {}/{}\".format(\n",
    "                        epoch, \n",
    "                        train_loss,\n",
    "                        val_loss,\n",
    "                        format_time(time.time() - tstart),\n",
    "                        i+1, \n",
    "                        len(train_dl)+1, \n",
    "                    ))\n",
    "    \n",
    "        # ========== Valid ==========\n",
    "        model.eval()\n",
    "        val_logits = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(valid_dl, disable=cfg.local_rank != 0):\n",
    "                x = x.to(cfg.local_rank)\n",
    "                y = y.to(cfg.local_rank)\n",
    "    \n",
    "                with autocast(device_type=cfg.device.type, dtype=torch.bfloat16):\n",
    "                    if ema_model is not None:\n",
    "                        out = ema_model.module(x)\n",
    "                    else:\n",
    "                        out = model(x)\n",
    "\n",
    "                val_logits.append(out.cpu())\n",
    "                val_targets.append(y.cpu())\n",
    "\n",
    "            val_logits= torch.cat(val_logits, dim=0)\n",
    "            val_targets= torch.cat(val_targets, dim=0)\n",
    "                \n",
    "            loss = criterion(val_logits, val_targets).item()\n",
    "\n",
    "\n",
    "        # Gather loss\n",
    "        v = torch.tensor([loss], device=cfg.local_rank)\n",
    "        torch.distributed.all_reduce(v, op=dist.ReduceOp.SUM)\n",
    "        val_loss = (v[0] / cfg.world_size).item()\n",
    "\n",
    "        scheduler.step(val_loss) \n",
    "\n",
    "        if cfg.local_rank == 0:\n",
    "            logging.info(f\"[Epoch {epoch}] lr: {scheduler.get_last_lr()[0]:.9e}\")\n",
    "    \n",
    "        # ========== Weights / Early stopping ==========\n",
    "        stop_train = torch.tensor([0], device=cfg.local_rank)\n",
    "        if cfg.local_rank == 0:\n",
    "            es= cfg.early_stopping\n",
    "            if val_loss < best_loss:\n",
    "                logging.info(\"New best: {:.2f} -> {:.2f}\".format(best_loss, val_loss))\n",
    "                logging.info(\"Saved weights..\")\n",
    "                best_loss = val_loss\n",
    "                if ema_model is not None:\n",
    "                    torch.save(ema_model.module.state_dict(), f'best_model_{cfg.seed}.pt')\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), f'best_model_{cfg.seed}.pt')\n",
    "        \n",
    "                es[\"streak\"] = 0\n",
    "            else:\n",
    "                es= cfg.early_stopping\n",
    "                es[\"streak\"] += 1\n",
    "                if es[\"streak\"] > es[\"patience\"]:\n",
    "                    logging.info(\"Ending training (early_stopping).\")\n",
    "                    stop_train = torch.tensor([1], device=cfg.local_rank)\n",
    "        \n",
    "        # Exits training on all ranks\n",
    "        dist.broadcast(stop_train, src=0)\n",
    "        if stop_train.item() == 1:\n",
    "            return\n",
    "\n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # GPU Specs\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    _, total = torch.cuda.mem_get_info(device=rank)\n",
    "\n",
    "    # Init\n",
    "    setup(rank, world_size)\n",
    "    time.sleep(rank)\n",
    "    print(f\"Rank: {rank}, World size: {world_size}, GPU memory: {total / 1024**3:.2f}GB\", flush=True)\n",
    "    time.sleep(world_size - rank)\n",
    "\n",
    "    # Seed\n",
    "    set_seed(cfg.seed+10)\n",
    "\n",
    "    # Run\n",
    "    cfg.local_rank= rank\n",
    "    cfg.world_size= world_size\n",
    "    main(cfg)\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "Rank: 0, World size: 2, GPU memory: 31.37GB\n",
      "Rank: 1, World size: 2, GPU memory: 31.37GB\n",
      "2025-06-26 09:15:02,016 =========================\n",
      "2025-06-26 09:15:02,017 Loading data..\n",
      "100%|███████████████████████████████████████| 920/920 [00:00<00:00, 1480.38it/s]\n",
      "100%|█████████████████████████████████████████| 20/20 [00:00<00:00, 1422.23it/s]\n",
      "2025-06-26 09:15:04,802 Loading pretrained weights from Hugging Face hub (timm/caformer_b36.sail_in22k_ft_in1k)\n",
      "2025-06-26 09:15:04,927 Loading pretrained weights from Hugging Face hub (timm/caformer_b36.sail_in22k_ft_in1k)\n",
      "2025-06-26 09:15:05,309 [timm/caformer_b36.sail_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-06-26 09:15:05,374 [timm/caformer_b36.sail_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-06-26 09:15:05,380 Converted input conv stem.conv pretrained weights from 3 to 5 channel(s)\n",
      "2025-06-26 09:15:05,398 Converted input conv stem.conv pretrained weights from 3 to 5 channel(s)\n",
      "2025-06-26 09:15:05,902 Missing keys (head.fc.fc2.weight, head.fc.fc2.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "2025-06-26 09:15:05,902 Missing keys (head.fc.fc2.weight, head.fc.fc2.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "Initializing EMA model..\n",
      "2025-06-26 09:15:08,468 =========================\n",
      "2025-06-26 09:15:08,468 Give me warp 2, Mr. Sulu.\n",
      "2025-06-26 09:15:08,468 =========================\n",
      "100%|█████████████████████████████████████████| 313/313 [00:51<00:00,  6.02it/s]\n",
      "2025-06-26 09:16:00,581 [Epoch 0] lr: 5.000000000e-06\n",
      "2025-06-26 09:16:00,581 New best: 1000000.00 -> 23.57\n",
      "2025-06-26 09:16:00,581 Saved weights..\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 512, 1, 1], strides() = [512, 1, 512, 512]\n",
      "bucket_view.sizes() = [256, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 512, 1, 1], strides() = [512, 1, 512, 512]\n",
      "bucket_view.sizes() = [256, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "2025-06-26 09:17:10,900 Epoch 1:     Train MAE: 12.47     Val MAE: 23.57     Time: 0:01:10     Step: 1/14376\n",
      "2025-06-26 09:17:38,593 Epoch 1:     Train MAE: 13.97     Val MAE: 23.57     Time: 0:01:38     Step: 101/14376\n",
      "2025-06-26 09:17:50,780 Epoch 1:     Train MAE: 13.41     Val MAE: 23.57     Time: 0:01:50     Step: 201/14376\n",
      "2025-06-26 09:18:02,391 Epoch 1:     Train MAE: 13.83     Val MAE: 23.57     Time: 0:02:01     Step: 301/14376\n",
      "2025-06-26 09:18:14,046 Epoch 1:     Train MAE: 13.90     Val MAE: 23.57     Time: 0:02:13     Step: 401/14376\n",
      "2025-06-26 09:18:25,653 Epoch 1:     Train MAE: 13.91     Val MAE: 23.57     Time: 0:02:25     Step: 501/14376\n",
      "2025-06-26 09:18:37,302 Epoch 1:     Train MAE: 13.47     Val MAE: 23.57     Time: 0:02:36     Step: 601/14376\n",
      "2025-06-26 09:18:48,937 Epoch 1:     Train MAE: 13.88     Val MAE: 23.57     Time: 0:02:48     Step: 701/14376\n",
      "2025-06-26 09:19:00,603 Epoch 1:     Train MAE: 13.44     Val MAE: 23.57     Time: 0:03:00     Step: 801/14376\n",
      "2025-06-26 09:19:12,246 Epoch 1:     Train MAE: 13.68     Val MAE: 23.57     Time: 0:03:11     Step: 901/14376\n",
      "2025-06-26 09:19:24,367 Epoch 1:     Train MAE: 13.86     Val MAE: 23.57     Time: 0:03:23     Step: 1001/14376\n",
      "2025-06-26 09:19:35,984 Epoch 1:     Train MAE: 14.01     Val MAE: 23.57     Time: 0:03:35     Step: 1101/14376\n",
      "2025-06-26 09:19:47,684 Epoch 1:     Train MAE: 13.66     Val MAE: 23.57     Time: 0:03:47     Step: 1201/14376\n",
      "2025-06-26 09:19:59,310 Epoch 1:     Train MAE: 13.82     Val MAE: 23.57     Time: 0:03:58     Step: 1301/14376\n",
      "2025-06-26 09:20:11,444 Epoch 1:     Train MAE: 13.79     Val MAE: 23.57     Time: 0:04:10     Step: 1401/14376\n",
      "2025-06-26 09:20:23,136 Epoch 1:     Train MAE: 13.46     Val MAE: 23.57     Time: 0:04:22     Step: 1501/14376\n",
      "2025-06-26 09:20:34,883 Epoch 1:     Train MAE: 13.48     Val MAE: 23.57     Time: 0:04:34     Step: 1601/14376\n",
      "2025-06-26 09:20:46,725 Epoch 1:     Train MAE: 13.16     Val MAE: 23.57     Time: 0:04:46     Step: 1701/14376\n",
      "2025-06-26 09:20:58,495 Epoch 1:     Train MAE: 13.88     Val MAE: 23.57     Time: 0:04:57     Step: 1801/14376\n",
      "2025-06-26 09:21:10,288 Epoch 1:     Train MAE: 13.32     Val MAE: 23.57     Time: 0:05:09     Step: 1901/14376\n",
      "2025-06-26 09:21:22,830 Epoch 1:     Train MAE: 13.55     Val MAE: 23.57     Time: 0:05:22     Step: 2001/14376\n",
      "2025-06-26 09:21:35,055 Epoch 1:     Train MAE: 13.59     Val MAE: 23.57     Time: 0:05:34     Step: 2101/14376\n",
      "2025-06-26 09:21:46,787 Epoch 1:     Train MAE: 13.78     Val MAE: 23.57     Time: 0:05:46     Step: 2201/14376\n",
      "2025-06-26 09:21:58,574 Epoch 1:     Train MAE: 13.91     Val MAE: 23.57     Time: 0:05:58     Step: 2301/14376\n",
      "2025-06-26 09:22:10,313 Epoch 1:     Train MAE: 13.48     Val MAE: 23.57     Time: 0:06:09     Step: 2401/14376\n",
      "2025-06-26 09:22:22,470 Epoch 1:     Train MAE: 13.88     Val MAE: 23.57     Time: 0:06:21     Step: 2501/14376\n",
      "2025-06-26 09:22:34,178 Epoch 1:     Train MAE: 13.45     Val MAE: 23.57     Time: 0:06:33     Step: 2601/14376\n",
      "2025-06-26 09:22:45,840 Epoch 1:     Train MAE: 13.55     Val MAE: 23.57     Time: 0:06:45     Step: 2701/14376\n",
      "2025-06-26 09:22:57,478 Epoch 1:     Train MAE: 13.46     Val MAE: 23.57     Time: 0:06:56     Step: 2801/14376\n",
      "2025-06-26 09:23:10,005 Epoch 1:     Train MAE: 13.73     Val MAE: 23.57     Time: 0:07:09     Step: 2901/14376\n",
      "2025-06-26 09:23:21,657 Epoch 1:     Train MAE: 13.65     Val MAE: 23.57     Time: 0:07:21     Step: 3001/14376\n",
      "2025-06-26 09:23:33,271 Epoch 1:     Train MAE: 13.41     Val MAE: 23.57     Time: 0:07:32     Step: 3101/14376\n",
      "2025-06-26 09:23:44,946 Epoch 1:     Train MAE: 13.50     Val MAE: 23.57     Time: 0:07:44     Step: 3201/14376\n",
      "2025-06-26 09:23:56,565 Epoch 1:     Train MAE: 13.49     Val MAE: 23.57     Time: 0:07:56     Step: 3301/14376\n",
      "2025-06-26 09:24:08,215 Epoch 1:     Train MAE: 13.62     Val MAE: 23.57     Time: 0:08:07     Step: 3401/14376\n",
      "2025-06-26 09:24:20,688 Epoch 1:     Train MAE: 13.84     Val MAE: 23.57     Time: 0:08:20     Step: 3501/14376\n",
      "2025-06-26 09:24:32,438 Epoch 1:     Train MAE: 13.75     Val MAE: 23.57     Time: 0:08:31     Step: 3601/14376\n",
      "2025-06-26 09:24:44,153 Epoch 1:     Train MAE: 13.69     Val MAE: 23.57     Time: 0:08:43     Step: 3701/14376\n",
      "2025-06-26 09:24:55,864 Epoch 1:     Train MAE: 13.86     Val MAE: 23.57     Time: 0:08:55     Step: 3801/14376\n",
      "2025-06-26 09:25:08,324 Epoch 1:     Train MAE: 13.42     Val MAE: 23.57     Time: 0:09:07     Step: 3901/14376\n",
      "2025-06-26 09:25:20,062 Epoch 1:     Train MAE: 13.72     Val MAE: 23.57     Time: 0:09:19     Step: 4001/14376\n",
      "2025-06-26 09:25:31,741 Epoch 1:     Train MAE: 13.47     Val MAE: 23.57     Time: 0:09:31     Step: 4101/14376\n",
      "2025-06-26 09:25:43,423 Epoch 1:     Train MAE: 13.50     Val MAE: 23.57     Time: 0:09:42     Step: 4201/14376\n",
      "2025-06-26 09:25:55,232 Epoch 1:     Train MAE: 13.87     Val MAE: 23.57     Time: 0:09:54     Step: 4301/14376\n",
      "2025-06-26 09:26:07,320 Epoch 1:     Train MAE: 13.57     Val MAE: 23.57     Time: 0:10:06     Step: 4401/14376\n",
      "2025-06-26 09:26:18,960 Epoch 1:     Train MAE: 13.78     Val MAE: 23.57     Time: 0:10:18     Step: 4501/14376\n",
      "2025-06-26 09:26:30,645 Epoch 1:     Train MAE: 13.96     Val MAE: 23.57     Time: 0:10:30     Step: 4601/14376\n",
      "2025-06-26 09:26:42,271 Epoch 1:     Train MAE: 13.46     Val MAE: 23.57     Time: 0:10:41     Step: 4701/14376\n",
      "2025-06-26 09:26:53,948 Epoch 1:     Train MAE: 13.86     Val MAE: 23.57     Time: 0:10:53     Step: 4801/14376\n",
      "2025-06-26 09:27:06,419 Epoch 1:     Train MAE: 13.64     Val MAE: 23.57     Time: 0:11:05     Step: 4901/14376\n",
      "2025-06-26 09:27:18,126 Epoch 1:     Train MAE: 13.70     Val MAE: 23.57     Time: 0:11:17     Step: 5001/14376\n",
      "2025-06-26 09:27:29,749 Epoch 1:     Train MAE: 13.70     Val MAE: 23.57     Time: 0:11:29     Step: 5101/14376\n",
      "2025-06-26 09:27:41,642 Epoch 1:     Train MAE: 13.88     Val MAE: 23.57     Time: 0:11:41     Step: 5201/14376\n",
      "2025-06-26 09:27:53,327 Epoch 1:     Train MAE: 13.27     Val MAE: 23.57     Time: 0:11:52     Step: 5301/14376\n",
      "2025-06-26 09:28:05,223 Epoch 1:     Train MAE: 13.31     Val MAE: 23.57     Time: 0:12:04     Step: 5401/14376\n",
      "2025-06-26 09:28:16,951 Epoch 1:     Train MAE: 13.81     Val MAE: 23.57     Time: 0:12:16     Step: 5501/14376\n",
      "2025-06-26 09:28:28,676 Epoch 1:     Train MAE: 13.68     Val MAE: 23.57     Time: 0:12:28     Step: 5601/14376\n",
      "2025-06-26 09:28:40,540 Epoch 1:     Train MAE: 13.72     Val MAE: 23.57     Time: 0:12:39     Step: 5701/14376\n",
      "2025-06-26 09:28:52,287 Epoch 1:     Train MAE: 14.03     Val MAE: 23.57     Time: 0:12:51     Step: 5801/14376\n",
      "2025-06-26 09:29:04,780 Epoch 1:     Train MAE: 13.67     Val MAE: 23.57     Time: 0:13:04     Step: 5901/14376\n",
      "2025-06-26 09:29:17,100 Epoch 1:     Train MAE: 13.27     Val MAE: 23.57     Time: 0:13:16     Step: 6001/14376\n",
      "2025-06-26 09:29:28,985 Epoch 1:     Train MAE: 13.59     Val MAE: 23.57     Time: 0:13:28     Step: 6101/14376\n",
      "2025-06-26 09:29:40,713 Epoch 1:     Train MAE: 13.77     Val MAE: 23.57     Time: 0:13:40     Step: 6201/14376\n",
      "2025-06-26 09:29:52,396 Epoch 1:     Train MAE: 13.57     Val MAE: 23.57     Time: 0:13:51     Step: 6301/14376\n",
      "2025-06-26 09:30:04,105 Epoch 1:     Train MAE: 13.99     Val MAE: 23.57     Time: 0:14:03     Step: 6401/14376\n",
      "2025-06-26 09:30:15,776 Epoch 1:     Train MAE: 13.71     Val MAE: 23.57     Time: 0:14:15     Step: 6501/14376\n",
      "2025-06-26 09:30:27,539 Epoch 1:     Train MAE: 13.44     Val MAE: 23.57     Time: 0:14:26     Step: 6601/14376\n",
      "2025-06-26 09:30:39,200 Epoch 1:     Train MAE: 13.55     Val MAE: 23.57     Time: 0:14:38     Step: 6701/14376\n",
      "2025-06-26 09:30:50,941 Epoch 1:     Train MAE: 13.60     Val MAE: 23.57     Time: 0:14:50     Step: 6801/14376\n",
      "2025-06-26 09:31:03,666 Epoch 1:     Train MAE: 13.33     Val MAE: 23.57     Time: 0:15:03     Step: 6901/14376\n",
      "2025-06-26 09:31:15,357 Epoch 1:     Train MAE: 14.06     Val MAE: 23.57     Time: 0:15:14     Step: 7001/14376\n",
      "2025-06-26 09:31:27,094 Epoch 1:     Train MAE: 13.47     Val MAE: 23.57     Time: 0:15:26     Step: 7101/14376\n",
      "2025-06-26 09:31:38,882 Epoch 1:     Train MAE: 13.62     Val MAE: 23.57     Time: 0:15:38     Step: 7201/14376\n",
      "2025-06-26 09:31:50,547 Epoch 1:     Train MAE: 13.49     Val MAE: 23.57     Time: 0:15:49     Step: 7301/14376\n",
      "2025-06-26 09:32:02,488 Epoch 1:     Train MAE: 13.43     Val MAE: 23.57     Time: 0:16:01     Step: 7401/14376\n",
      "2025-06-26 09:32:14,200 Epoch 1:     Train MAE: 13.77     Val MAE: 23.57     Time: 0:16:13     Step: 7501/14376\n",
      "2025-06-26 09:32:25,918 Epoch 1:     Train MAE: 13.54     Val MAE: 23.57     Time: 0:16:25     Step: 7601/14376\n",
      "2025-06-26 09:32:37,696 Epoch 1:     Train MAE: 13.41     Val MAE: 23.57     Time: 0:16:37     Step: 7701/14376\n",
      "2025-06-26 09:32:50,268 Epoch 1:     Train MAE: 13.36     Val MAE: 23.57     Time: 0:16:49     Step: 7801/14376\n",
      "2025-06-26 09:33:02,051 Epoch 1:     Train MAE: 13.74     Val MAE: 23.57     Time: 0:17:01     Step: 7901/14376\n",
      "2025-06-26 09:33:13,916 Epoch 1:     Train MAE: 13.44     Val MAE: 23.57     Time: 0:17:13     Step: 8001/14376\n",
      "2025-06-26 09:33:25,686 Epoch 1:     Train MAE: 13.73     Val MAE: 23.57     Time: 0:17:25     Step: 8101/14376\n",
      "2025-06-26 09:33:37,531 Epoch 1:     Train MAE: 13.57     Val MAE: 23.57     Time: 0:17:36     Step: 8201/14376\n",
      "2025-06-26 09:33:49,271 Epoch 1:     Train MAE: 13.86     Val MAE: 23.57     Time: 0:17:48     Step: 8301/14376\n",
      "2025-06-26 09:34:01,491 Epoch 1:     Train MAE: 13.51     Val MAE: 23.57     Time: 0:18:00     Step: 8401/14376\n",
      "2025-06-26 09:34:13,355 Epoch 1:     Train MAE: 13.90     Val MAE: 23.57     Time: 0:18:12     Step: 8501/14376\n",
      "2025-06-26 09:34:25,131 Epoch 1:     Train MAE: 13.96     Val MAE: 23.57     Time: 0:18:24     Step: 8601/14376\n",
      "2025-06-26 09:34:36,872 Epoch 1:     Train MAE: 13.58     Val MAE: 23.57     Time: 0:18:36     Step: 8701/14376\n",
      "2025-06-26 09:34:49,448 Epoch 1:     Train MAE: 13.45     Val MAE: 23.57     Time: 0:18:48     Step: 8801/14376\n",
      "2025-06-26 09:35:01,368 Epoch 1:     Train MAE: 13.50     Val MAE: 23.57     Time: 0:19:00     Step: 8901/14376\n",
      "2025-06-26 09:35:13,085 Epoch 1:     Train MAE: 13.28     Val MAE: 23.57     Time: 0:19:12     Step: 9001/14376\n",
      "2025-06-26 09:35:25,407 Epoch 1:     Train MAE: 13.37     Val MAE: 23.57     Time: 0:19:24     Step: 9101/14376\n",
      "2025-06-26 09:35:37,099 Epoch 1:     Train MAE: 13.60     Val MAE: 23.57     Time: 0:19:36     Step: 9201/14376\n",
      "2025-06-26 09:35:48,868 Epoch 1:     Train MAE: 13.55     Val MAE: 23.57     Time: 0:19:48     Step: 9301/14376\n",
      "2025-06-26 09:36:00,618 Epoch 1:     Train MAE: 13.69     Val MAE: 23.57     Time: 0:20:00     Step: 9401/14376\n",
      "2025-06-26 09:36:12,330 Epoch 1:     Train MAE: 13.75     Val MAE: 23.57     Time: 0:20:11     Step: 9501/14376\n",
      "2025-06-26 09:36:24,049 Epoch 1:     Train MAE: 13.48     Val MAE: 23.57     Time: 0:20:23     Step: 9601/14376\n",
      "2025-06-26 09:36:35,959 Epoch 1:     Train MAE: 13.24     Val MAE: 23.57     Time: 0:20:35     Step: 9701/14376\n",
      "2025-06-26 09:36:48,463 Epoch 1:     Train MAE: 13.11     Val MAE: 23.57     Time: 0:20:47     Step: 9801/14376\n",
      "2025-06-26 09:37:00,213 Epoch 1:     Train MAE: 13.49     Val MAE: 23.57     Time: 0:20:59     Step: 9901/14376\n",
      "2025-06-26 09:37:11,990 Epoch 1:     Train MAE: 14.11     Val MAE: 23.57     Time: 0:21:11     Step: 10001/14376\n",
      "2025-06-26 09:37:23,947 Epoch 1:     Train MAE: 13.08     Val MAE: 23.57     Time: 0:21:23     Step: 10101/14376\n",
      "2025-06-26 09:37:35,653 Epoch 1:     Train MAE: 13.54     Val MAE: 23.57     Time: 0:21:35     Step: 10201/14376\n",
      "2025-06-26 09:37:47,384 Epoch 1:     Train MAE: 13.57     Val MAE: 23.57     Time: 0:21:46     Step: 10301/14376\n",
      "2025-06-26 09:37:59,174 Epoch 1:     Train MAE: 14.15     Val MAE: 23.57     Time: 0:21:58     Step: 10401/14376\n",
      "2025-06-26 09:38:11,179 Epoch 1:     Train MAE: 13.01     Val MAE: 23.57     Time: 0:22:10     Step: 10501/14376\n",
      "2025-06-26 09:38:22,863 Epoch 1:     Train MAE: 13.41     Val MAE: 23.57     Time: 0:22:22     Step: 10601/14376\n",
      "2025-06-26 09:38:34,832 Epoch 1:     Train MAE: 13.67     Val MAE: 23.57     Time: 0:22:34     Step: 10701/14376\n",
      "2025-06-26 09:38:47,455 Epoch 1:     Train MAE: 13.25     Val MAE: 23.57     Time: 0:22:46     Step: 10801/14376\n",
      "2025-06-26 09:38:59,112 Epoch 1:     Train MAE: 13.41     Val MAE: 23.57     Time: 0:22:58     Step: 10901/14376\n",
      "2025-06-26 09:39:10,894 Epoch 1:     Train MAE: 13.22     Val MAE: 23.57     Time: 0:23:10     Step: 11001/14376\n",
      "2025-06-26 09:39:22,581 Epoch 1:     Train MAE: 13.12     Val MAE: 23.57     Time: 0:23:22     Step: 11101/14376\n",
      "2025-06-26 09:39:34,317 Epoch 1:     Train MAE: 13.74     Val MAE: 23.57     Time: 0:23:33     Step: 11201/14376\n",
      "2025-06-26 09:39:45,989 Epoch 1:     Train MAE: 13.66     Val MAE: 23.57     Time: 0:23:45     Step: 11301/14376\n",
      "2025-06-26 09:39:57,730 Epoch 1:     Train MAE: 13.70     Val MAE: 23.57     Time: 0:23:57     Step: 11401/14376\n",
      "2025-06-26 09:40:09,793 Epoch 1:     Train MAE: 13.69     Val MAE: 23.57     Time: 0:24:09     Step: 11501/14376\n",
      "2025-06-26 09:40:21,868 Epoch 1:     Train MAE: 13.65     Val MAE: 23.57     Time: 0:24:21     Step: 11601/14376\n",
      "2025-06-26 09:40:34,136 Epoch 1:     Train MAE: 13.27     Val MAE: 23.57     Time: 0:24:33     Step: 11701/14376\n",
      "2025-06-26 09:40:46,519 Epoch 1:     Train MAE: 13.57     Val MAE: 23.57     Time: 0:24:45     Step: 11801/14376\n",
      "2025-06-26 09:40:58,293 Epoch 1:     Train MAE: 13.32     Val MAE: 23.57     Time: 0:24:57     Step: 11901/14376\n",
      "2025-06-26 09:41:09,976 Epoch 1:     Train MAE: 13.27     Val MAE: 23.57     Time: 0:25:09     Step: 12001/14376\n",
      "2025-06-26 09:41:21,751 Epoch 1:     Train MAE: 13.37     Val MAE: 23.57     Time: 0:25:21     Step: 12101/14376\n",
      "2025-06-26 09:41:33,477 Epoch 1:     Train MAE: 13.28     Val MAE: 23.57     Time: 0:25:32     Step: 12201/14376\n",
      "2025-06-26 09:41:45,197 Epoch 1:     Train MAE: 13.86     Val MAE: 23.57     Time: 0:25:44     Step: 12301/14376\n",
      "2025-06-26 09:41:57,049 Epoch 1:     Train MAE: 13.63     Val MAE: 23.57     Time: 0:25:56     Step: 12401/14376\n",
      "2025-06-26 09:42:08,818 Epoch 1:     Train MAE: 13.42     Val MAE: 23.57     Time: 0:26:08     Step: 12501/14376\n",
      "2025-06-26 09:42:20,511 Epoch 1:     Train MAE: 13.38     Val MAE: 23.57     Time: 0:26:19     Step: 12601/14376\n",
      "2025-06-26 09:42:33,276 Epoch 1:     Train MAE: 13.61     Val MAE: 23.57     Time: 0:26:32     Step: 12701/14376\n",
      "2025-06-26 09:42:44,968 Epoch 1:     Train MAE: 13.11     Val MAE: 23.57     Time: 0:26:44     Step: 12801/14376\n",
      "2025-06-26 09:42:56,822 Epoch 1:     Train MAE: 13.49     Val MAE: 23.57     Time: 0:26:56     Step: 12901/14376\n",
      "2025-06-26 09:43:08,501 Epoch 1:     Train MAE: 13.35     Val MAE: 23.57     Time: 0:27:07     Step: 13001/14376\n",
      "2025-06-26 09:43:20,453 Epoch 1:     Train MAE: 13.54     Val MAE: 23.57     Time: 0:27:19     Step: 13101/14376\n",
      "2025-06-26 09:43:32,166 Epoch 1:     Train MAE: 13.98     Val MAE: 23.57     Time: 0:27:31     Step: 13201/14376\n",
      "2025-06-26 09:43:44,230 Epoch 1:     Train MAE: 13.77     Val MAE: 23.57     Time: 0:27:43     Step: 13301/14376\n",
      "2025-06-26 09:43:55,859 Epoch 1:     Train MAE: 13.37     Val MAE: 23.57     Time: 0:27:55     Step: 13401/14376\n",
      "2025-06-26 09:44:07,589 Epoch 1:     Train MAE: 13.40     Val MAE: 23.57     Time: 0:28:07     Step: 13501/14376\n",
      "2025-06-26 09:44:19,234 Epoch 1:     Train MAE: 13.44     Val MAE: 23.57     Time: 0:28:18     Step: 13601/14376\n",
      "2025-06-26 09:44:31,740 Epoch 1:     Train MAE: 13.60     Val MAE: 23.57     Time: 0:28:31     Step: 13701/14376\n",
      "2025-06-26 09:44:43,396 Epoch 1:     Train MAE: 13.88     Val MAE: 23.57     Time: 0:28:42     Step: 13801/14376\n",
      "2025-06-26 09:44:55,111 Epoch 1:     Train MAE: 13.26     Val MAE: 23.57     Time: 0:28:54     Step: 13901/14376\n",
      "2025-06-26 09:45:06,769 Epoch 1:     Train MAE: 13.98     Val MAE: 23.57     Time: 0:29:06     Step: 14001/14376\n",
      "2025-06-26 09:45:18,594 Epoch 1:     Train MAE: 13.46     Val MAE: 23.57     Time: 0:29:18     Step: 14101/14376\n",
      "2025-06-26 09:45:30,391 Epoch 1:     Train MAE: 13.19     Val MAE: 23.57     Time: 0:29:29     Step: 14201/14376\n",
      "2025-06-26 09:45:42,303 Epoch 1:     Train MAE: 13.56     Val MAE: 23.57     Time: 0:29:41     Step: 14301/14376\n",
      "100%|█████████████████████████████████████████| 313/313 [00:10<00:00, 30.22it/s]\n",
      "2025-06-26 09:46:03,767 [Epoch 1] lr: 5.000000000e-06\n",
      "2025-06-26 09:46:03,767 New best: 23.57 -> 23.54\n",
      "2025-06-26 09:46:03,767 Saved weights..\n",
      "2025-06-26 09:46:04,570 Epoch 2:     Train MAE: 11.78     Val MAE: 23.54     Time: 0:00:00     Step: 1/14376\n",
      "2025-06-26 09:46:16,162 Epoch 2:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:00:12     Step: 101/14376\n",
      "2025-06-26 09:46:27,835 Epoch 2:     Train MAE: 13.35     Val MAE: 23.54     Time: 0:00:24     Step: 201/14376\n",
      "2025-06-26 09:46:39,441 Epoch 2:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:00:35     Step: 301/14376\n",
      "2025-06-26 09:46:51,374 Epoch 2:     Train MAE: 13.25     Val MAE: 23.54     Time: 0:00:47     Step: 401/14376\n",
      "2025-06-26 09:47:03,065 Epoch 2:     Train MAE: 13.98     Val MAE: 23.54     Time: 0:00:59     Step: 501/14376\n",
      "2025-06-26 09:47:14,774 Epoch 2:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:01:11     Step: 601/14376\n",
      "2025-06-26 09:47:26,454 Epoch 2:     Train MAE: 13.33     Val MAE: 23.54     Time: 0:01:22     Step: 701/14376\n",
      "2025-06-26 09:47:38,130 Epoch 2:     Train MAE: 13.33     Val MAE: 23.54     Time: 0:01:34     Step: 801/14376\n",
      "2025-06-26 09:47:50,153 Epoch 2:     Train MAE: 13.02     Val MAE: 23.54     Time: 0:01:46     Step: 901/14376\n",
      "2025-06-26 09:48:03,062 Epoch 2:     Train MAE: 13.24     Val MAE: 23.54     Time: 0:01:59     Step: 1001/14376\n",
      "2025-06-26 09:48:14,895 Epoch 2:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:02:11     Step: 1101/14376\n",
      "2025-06-26 09:48:26,561 Epoch 2:     Train MAE: 13.64     Val MAE: 23.54     Time: 0:02:22     Step: 1201/14376\n",
      "2025-06-26 09:48:38,353 Epoch 2:     Train MAE: 13.59     Val MAE: 23.54     Time: 0:02:34     Step: 1301/14376\n",
      "2025-06-26 09:48:50,603 Epoch 2:     Train MAE: 13.66     Val MAE: 23.54     Time: 0:02:46     Step: 1401/14376\n",
      "2025-06-26 09:49:02,234 Epoch 2:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:02:58     Step: 1501/14376\n",
      "2025-06-26 09:49:14,213 Epoch 2:     Train MAE: 13.66     Val MAE: 23.54     Time: 0:03:10     Step: 1601/14376\n",
      "2025-06-26 09:49:25,861 Epoch 2:     Train MAE: 13.73     Val MAE: 23.54     Time: 0:03:22     Step: 1701/14376\n",
      "2025-06-26 09:49:37,576 Epoch 2:     Train MAE: 13.71     Val MAE: 23.54     Time: 0:03:33     Step: 1801/14376\n",
      "2025-06-26 09:49:49,698 Epoch 2:     Train MAE: 13.66     Val MAE: 23.54     Time: 0:03:46     Step: 1901/14376\n",
      "2025-06-26 09:50:02,371 Epoch 2:     Train MAE: 13.54     Val MAE: 23.54     Time: 0:03:58     Step: 2001/14376\n",
      "2025-06-26 09:50:14,242 Epoch 2:     Train MAE: 13.48     Val MAE: 23.54     Time: 0:04:10     Step: 2101/14376\n",
      "2025-06-26 09:50:25,969 Epoch 2:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:04:22     Step: 2201/14376\n",
      "2025-06-26 09:50:37,612 Epoch 2:     Train MAE: 13.52     Val MAE: 23.54     Time: 0:04:33     Step: 2301/14376\n",
      "2025-06-26 09:50:49,346 Epoch 2:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:04:45     Step: 2401/14376\n",
      "2025-06-26 09:51:00,988 Epoch 2:     Train MAE: 13.68     Val MAE: 23.54     Time: 0:04:57     Step: 2501/14376\n",
      "2025-06-26 09:51:12,698 Epoch 2:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:05:09     Step: 2601/14376\n",
      "2025-06-26 09:51:24,799 Epoch 2:     Train MAE: 14.11     Val MAE: 23.54     Time: 0:05:21     Step: 2701/14376\n",
      "2025-06-26 09:51:36,715 Epoch 2:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:05:33     Step: 2801/14376\n",
      "2025-06-26 09:51:48,809 Epoch 2:     Train MAE: 13.10     Val MAE: 23.54     Time: 0:05:45     Step: 2901/14376\n",
      "2025-06-26 09:52:00,942 Epoch 2:     Train MAE: 13.64     Val MAE: 23.54     Time: 0:05:57     Step: 3001/14376\n",
      "2025-06-26 09:52:12,573 Epoch 2:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:06:08     Step: 3101/14376\n",
      "2025-06-26 09:52:24,319 Epoch 2:     Train MAE: 13.66     Val MAE: 23.54     Time: 0:06:20     Step: 3201/14376\n",
      "2025-06-26 09:52:36,599 Epoch 2:     Train MAE: 13.41     Val MAE: 23.54     Time: 0:06:32     Step: 3301/14376\n",
      "2025-06-26 09:52:48,633 Epoch 2:     Train MAE: 14.26     Val MAE: 23.54     Time: 0:06:44     Step: 3401/14376\n",
      "2025-06-26 09:53:00,619 Epoch 2:     Train MAE: 13.81     Val MAE: 23.54     Time: 0:06:56     Step: 3501/14376\n",
      "2025-06-26 09:53:12,610 Epoch 2:     Train MAE: 13.71     Val MAE: 23.54     Time: 0:07:08     Step: 3601/14376\n",
      "2025-06-26 09:53:24,271 Epoch 2:     Train MAE: 13.32     Val MAE: 23.54     Time: 0:07:20     Step: 3701/14376\n",
      "2025-06-26 09:53:35,907 Epoch 2:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:07:32     Step: 3801/14376\n",
      "2025-06-26 09:53:48,385 Epoch 2:     Train MAE: 13.28     Val MAE: 23.54     Time: 0:07:44     Step: 3901/14376\n",
      "2025-06-26 09:54:00,065 Epoch 2:     Train MAE: 13.36     Val MAE: 23.54     Time: 0:07:56     Step: 4001/14376\n",
      "2025-06-26 09:54:11,784 Epoch 2:     Train MAE: 13.48     Val MAE: 23.54     Time: 0:08:08     Step: 4101/14376\n",
      "2025-06-26 09:54:23,409 Epoch 2:     Train MAE: 13.58     Val MAE: 23.54     Time: 0:08:19     Step: 4201/14376\n",
      "2025-06-26 09:54:35,416 Epoch 2:     Train MAE: 13.88     Val MAE: 23.54     Time: 0:08:31     Step: 4301/14376\n",
      "2025-06-26 09:54:47,061 Epoch 2:     Train MAE: 13.24     Val MAE: 23.54     Time: 0:08:43     Step: 4401/14376\n",
      "2025-06-26 09:54:58,769 Epoch 2:     Train MAE: 13.93     Val MAE: 23.54     Time: 0:08:55     Step: 4501/14376\n",
      "2025-06-26 09:55:10,457 Epoch 2:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:09:06     Step: 4601/14376\n",
      "2025-06-26 09:55:22,182 Epoch 2:     Train MAE: 13.91     Val MAE: 23.54     Time: 0:09:18     Step: 4701/14376\n",
      "2025-06-26 09:55:33,857 Epoch 2:     Train MAE: 13.26     Val MAE: 23.54     Time: 0:09:30     Step: 4801/14376\n",
      "2025-06-26 09:55:46,365 Epoch 2:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:09:42     Step: 4901/14376\n",
      "2025-06-26 09:55:57,987 Epoch 2:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:09:54     Step: 5001/14376\n",
      "2025-06-26 09:56:09,657 Epoch 2:     Train MAE: 13.16     Val MAE: 23.54     Time: 0:10:05     Step: 5101/14376\n",
      "2025-06-26 09:56:21,484 Epoch 2:     Train MAE: 13.81     Val MAE: 23.54     Time: 0:10:17     Step: 5201/14376\n",
      "2025-06-26 09:56:33,089 Epoch 2:     Train MAE: 13.16     Val MAE: 23.54     Time: 0:10:29     Step: 5301/14376\n",
      "2025-06-26 09:56:44,694 Epoch 2:     Train MAE: 13.67     Val MAE: 23.54     Time: 0:10:41     Step: 5401/14376\n",
      "2025-06-26 09:56:56,880 Epoch 2:     Train MAE: 13.46     Val MAE: 23.54     Time: 0:10:53     Step: 5501/14376\n",
      "2025-06-26 09:57:08,469 Epoch 2:     Train MAE: 13.39     Val MAE: 23.54     Time: 0:11:04     Step: 5601/14376\n",
      "2025-06-26 09:57:20,258 Epoch 2:     Train MAE: 13.26     Val MAE: 23.54     Time: 0:11:16     Step: 5701/14376\n",
      "2025-06-26 09:57:31,846 Epoch 2:     Train MAE: 13.09     Val MAE: 23.54     Time: 0:11:28     Step: 5801/14376\n",
      "2025-06-26 09:57:44,215 Epoch 2:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:11:40     Step: 5901/14376\n",
      "2025-06-26 09:57:55,810 Epoch 2:     Train MAE: 13.28     Val MAE: 23.54     Time: 0:11:52     Step: 6001/14376\n",
      "2025-06-26 09:58:07,399 Epoch 2:     Train MAE: 14.01     Val MAE: 23.54     Time: 0:12:03     Step: 6101/14376\n",
      "2025-06-26 09:58:19,019 Epoch 2:     Train MAE: 13.73     Val MAE: 23.54     Time: 0:12:15     Step: 6201/14376\n",
      "2025-06-26 09:58:30,596 Epoch 2:     Train MAE: 13.62     Val MAE: 23.54     Time: 0:12:26     Step: 6301/14376\n",
      "2025-06-26 09:58:42,247 Epoch 2:     Train MAE: 14.12     Val MAE: 23.54     Time: 0:12:38     Step: 6401/14376\n",
      "2025-06-26 09:58:53,843 Epoch 2:     Train MAE: 13.27     Val MAE: 23.54     Time: 0:12:50     Step: 6501/14376\n",
      "2025-06-26 09:59:05,468 Epoch 2:     Train MAE: 13.32     Val MAE: 23.54     Time: 0:13:01     Step: 6601/14376\n",
      "2025-06-26 09:59:17,093 Epoch 2:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:13:13     Step: 6701/14376\n",
      "2025-06-26 09:59:28,736 Epoch 2:     Train MAE: 13.61     Val MAE: 23.54     Time: 0:13:25     Step: 6801/14376\n",
      "2025-06-26 09:59:41,160 Epoch 2:     Train MAE: 13.52     Val MAE: 23.54     Time: 0:13:37     Step: 6901/14376\n",
      "2025-06-26 09:59:52,767 Epoch 2:     Train MAE: 13.59     Val MAE: 23.54     Time: 0:13:49     Step: 7001/14376\n",
      "2025-06-26 10:00:04,398 Epoch 2:     Train MAE: 13.40     Val MAE: 23.54     Time: 0:14:00     Step: 7101/14376\n",
      "2025-06-26 10:00:15,989 Epoch 2:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:14:12     Step: 7201/14376\n",
      "2025-06-26 10:00:27,589 Epoch 2:     Train MAE: 13.28     Val MAE: 23.54     Time: 0:14:23     Step: 7301/14376\n",
      "2025-06-26 10:00:39,193 Epoch 2:     Train MAE: 13.67     Val MAE: 23.54     Time: 0:14:35     Step: 7401/14376\n",
      "2025-06-26 10:00:50,788 Epoch 2:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:14:47     Step: 7501/14376\n",
      "2025-06-26 10:01:02,410 Epoch 2:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:14:58     Step: 7601/14376\n",
      "2025-06-26 10:01:14,021 Epoch 2:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:15:10     Step: 7701/14376\n",
      "2025-06-26 10:01:25,622 Epoch 2:     Train MAE: 13.59     Val MAE: 23.54     Time: 0:15:21     Step: 7801/14376\n",
      "2025-06-26 10:01:37,957 Epoch 2:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:15:34     Step: 7901/14376\n",
      "2025-06-26 10:01:49,570 Epoch 2:     Train MAE: 13.91     Val MAE: 23.54     Time: 0:15:45     Step: 8001/14376\n",
      "2025-06-26 10:02:01,277 Epoch 2:     Train MAE: 13.46     Val MAE: 23.54     Time: 0:15:57     Step: 8101/14376\n",
      "2025-06-26 10:02:12,866 Epoch 2:     Train MAE: 13.19     Val MAE: 23.54     Time: 0:16:09     Step: 8201/14376\n",
      "2025-06-26 10:02:24,458 Epoch 2:     Train MAE: 13.15     Val MAE: 23.54     Time: 0:16:20     Step: 8301/14376\n",
      "2025-06-26 10:02:36,089 Epoch 2:     Train MAE: 13.88     Val MAE: 23.54     Time: 0:16:32     Step: 8401/14376\n",
      "2025-06-26 10:02:47,736 Epoch 2:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:16:44     Step: 8501/14376\n",
      "2025-06-26 10:02:59,397 Epoch 2:     Train MAE: 13.46     Val MAE: 23.54     Time: 0:16:55     Step: 8601/14376\n",
      "2025-06-26 10:03:11,025 Epoch 2:     Train MAE: 13.42     Val MAE: 23.54     Time: 0:17:07     Step: 8701/14376\n",
      "2025-06-26 10:03:23,082 Epoch 2:     Train MAE: 13.41     Val MAE: 23.54     Time: 0:17:19     Step: 8801/14376\n",
      "2025-06-26 10:03:35,147 Epoch 2:     Train MAE: 13.25     Val MAE: 23.54     Time: 0:17:31     Step: 8901/14376\n",
      "2025-06-26 10:03:46,850 Epoch 2:     Train MAE: 13.13     Val MAE: 23.54     Time: 0:17:43     Step: 9001/14376\n",
      "2025-06-26 10:03:58,495 Epoch 2:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:17:54     Step: 9101/14376\n",
      "2025-06-26 10:04:10,145 Epoch 2:     Train MAE: 13.52     Val MAE: 23.54     Time: 0:18:06     Step: 9201/14376\n",
      "2025-06-26 10:04:21,826 Epoch 2:     Train MAE: 12.90     Val MAE: 23.54     Time: 0:18:18     Step: 9301/14376\n",
      "2025-06-26 10:04:33,488 Epoch 2:     Train MAE: 13.37     Val MAE: 23.54     Time: 0:18:29     Step: 9401/14376\n",
      "2025-06-26 10:04:45,142 Epoch 2:     Train MAE: 13.27     Val MAE: 23.54     Time: 0:18:41     Step: 9501/14376\n",
      "2025-06-26 10:04:56,821 Epoch 2:     Train MAE: 14.08     Val MAE: 23.54     Time: 0:18:53     Step: 9601/14376\n",
      "2025-06-26 10:05:08,472 Epoch 2:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:19:04     Step: 9701/14376\n",
      "2025-06-26 10:05:20,990 Epoch 2:     Train MAE: 12.93     Val MAE: 23.54     Time: 0:19:17     Step: 9801/14376\n",
      "2025-06-26 10:05:32,897 Epoch 2:     Train MAE: 13.42     Val MAE: 23.54     Time: 0:19:29     Step: 9901/14376\n",
      "2025-06-26 10:05:44,552 Epoch 2:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:19:40     Step: 10001/14376\n",
      "2025-06-26 10:05:56,517 Epoch 2:     Train MAE: 13.44     Val MAE: 23.54     Time: 0:19:52     Step: 10101/14376\n",
      "2025-06-26 10:06:08,202 Epoch 2:     Train MAE: 13.32     Val MAE: 23.54     Time: 0:20:04     Step: 10201/14376\n",
      "2025-06-26 10:06:19,918 Epoch 2:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:20:16     Step: 10301/14376\n",
      "2025-06-26 10:06:32,178 Epoch 2:     Train MAE: 13.21     Val MAE: 23.54     Time: 0:20:28     Step: 10401/14376\n",
      "2025-06-26 10:06:43,931 Epoch 2:     Train MAE: 13.85     Val MAE: 23.54     Time: 0:20:40     Step: 10501/14376\n",
      "2025-06-26 10:06:55,666 Epoch 2:     Train MAE: 13.87     Val MAE: 23.54     Time: 0:20:51     Step: 10601/14376\n",
      "2025-06-26 10:07:08,302 Epoch 2:     Train MAE: 13.92     Val MAE: 23.54     Time: 0:21:04     Step: 10701/14376\n",
      "2025-06-26 10:07:20,935 Epoch 2:     Train MAE: 13.25     Val MAE: 23.54     Time: 0:21:17     Step: 10801/14376\n",
      "2025-06-26 10:07:32,645 Epoch 2:     Train MAE: 13.39     Val MAE: 23.54     Time: 0:21:28     Step: 10901/14376\n",
      "2025-06-26 10:07:44,311 Epoch 2:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:21:40     Step: 11001/14376\n",
      "2025-06-26 10:07:56,044 Epoch 2:     Train MAE: 13.55     Val MAE: 23.54     Time: 0:21:52     Step: 11101/14376\n",
      "2025-06-26 10:08:07,703 Epoch 2:     Train MAE: 13.61     Val MAE: 23.54     Time: 0:22:04     Step: 11201/14376\n",
      "2025-06-26 10:08:19,412 Epoch 2:     Train MAE: 13.18     Val MAE: 23.54     Time: 0:22:15     Step: 11301/14376\n",
      "2025-06-26 10:08:31,024 Epoch 2:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:22:27     Step: 11401/14376\n",
      "2025-06-26 10:08:42,799 Epoch 2:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:22:39     Step: 11501/14376\n",
      "2025-06-26 10:08:54,554 Epoch 2:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:22:50     Step: 11601/14376\n",
      "2025-06-26 10:09:06,249 Epoch 2:     Train MAE: 13.76     Val MAE: 23.54     Time: 0:23:02     Step: 11701/14376\n",
      "2025-06-26 10:09:18,683 Epoch 2:     Train MAE: 13.40     Val MAE: 23.54     Time: 0:23:14     Step: 11801/14376\n",
      "2025-06-26 10:09:30,350 Epoch 2:     Train MAE: 13.41     Val MAE: 23.54     Time: 0:23:26     Step: 11901/14376\n",
      "2025-06-26 10:09:41,984 Epoch 2:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:23:38     Step: 12001/14376\n",
      "2025-06-26 10:09:53,682 Epoch 2:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:23:49     Step: 12101/14376\n",
      "2025-06-26 10:10:05,309 Epoch 2:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:24:01     Step: 12201/14376\n",
      "2025-06-26 10:10:17,010 Epoch 2:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:24:13     Step: 12301/14376\n",
      "2025-06-26 10:10:28,641 Epoch 2:     Train MAE: 13.22     Val MAE: 23.54     Time: 0:24:24     Step: 12401/14376\n",
      "2025-06-26 10:10:40,946 Epoch 2:     Train MAE: 13.66     Val MAE: 23.54     Time: 0:24:37     Step: 12501/14376\n",
      "2025-06-26 10:10:52,591 Epoch 2:     Train MAE: 13.74     Val MAE: 23.54     Time: 0:24:48     Step: 12601/14376\n",
      "2025-06-26 10:11:04,300 Epoch 2:     Train MAE: 13.59     Val MAE: 23.54     Time: 0:25:00     Step: 12701/14376\n",
      "2025-06-26 10:11:16,755 Epoch 2:     Train MAE: 13.27     Val MAE: 23.54     Time: 0:25:13     Step: 12801/14376\n",
      "2025-06-26 10:11:28,503 Epoch 2:     Train MAE: 13.36     Val MAE: 23.54     Time: 0:25:24     Step: 12901/14376\n",
      "2025-06-26 10:11:40,115 Epoch 2:     Train MAE: 13.89     Val MAE: 23.54     Time: 0:25:36     Step: 13001/14376\n",
      "2025-06-26 10:11:51,824 Epoch 2:     Train MAE: 13.67     Val MAE: 23.54     Time: 0:25:48     Step: 13101/14376\n",
      "2025-06-26 10:12:03,616 Epoch 2:     Train MAE: 13.26     Val MAE: 23.54     Time: 0:25:59     Step: 13201/14376\n",
      "2025-06-26 10:12:15,350 Epoch 2:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:26:11     Step: 13301/14376\n",
      "2025-06-26 10:12:27,065 Epoch 2:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:26:23     Step: 13401/14376\n",
      "2025-06-26 10:12:38,841 Epoch 2:     Train MAE: 13.46     Val MAE: 23.54     Time: 0:26:35     Step: 13501/14376\n",
      "2025-06-26 10:12:50,539 Epoch 2:     Train MAE: 13.33     Val MAE: 23.54     Time: 0:26:46     Step: 13601/14376\n",
      "2025-06-26 10:13:02,677 Epoch 2:     Train MAE: 13.15     Val MAE: 23.54     Time: 0:26:58     Step: 13701/14376\n",
      "2025-06-26 10:13:14,800 Epoch 2:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:27:11     Step: 13801/14376\n",
      "2025-06-26 10:13:26,537 Epoch 2:     Train MAE: 13.52     Val MAE: 23.54     Time: 0:27:22     Step: 13901/14376\n",
      "2025-06-26 10:13:38,236 Epoch 2:     Train MAE: 14.10     Val MAE: 23.54     Time: 0:27:34     Step: 14001/14376\n",
      "2025-06-26 10:13:50,209 Epoch 2:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:27:46     Step: 14101/14376\n",
      "2025-06-26 10:14:01,902 Epoch 2:     Train MAE: 13.61     Val MAE: 23.54     Time: 0:27:58     Step: 14201/14376\n",
      "2025-06-26 10:14:13,662 Epoch 2:     Train MAE: 13.71     Val MAE: 23.54     Time: 0:28:09     Step: 14301/14376\n",
      "100%|█████████████████████████████████████████| 313/313 [00:09<00:00, 32.13it/s]\n",
      "2025-06-26 10:14:35,262 [Epoch 2] lr: 5.000000000e-06\n",
      "2025-06-26 10:14:35,715 Epoch 3:     Train MAE: 15.21     Val MAE: 23.54     Time: 0:00:00     Step: 1/14376\n",
      "2025-06-26 10:14:47,613 Epoch 3:     Train MAE: 13.74     Val MAE: 23.54     Time: 0:00:12     Step: 101/14376\n",
      "2025-06-26 10:14:59,399 Epoch 3:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:00:24     Step: 201/14376\n",
      "2025-06-26 10:15:11,528 Epoch 3:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:00:36     Step: 301/14376\n",
      "2025-06-26 10:15:23,731 Epoch 3:     Train MAE: 13.39     Val MAE: 23.54     Time: 0:00:48     Step: 401/14376\n",
      "2025-06-26 10:15:35,404 Epoch 3:     Train MAE: 13.35     Val MAE: 23.54     Time: 0:01:00     Step: 501/14376\n",
      "2025-06-26 10:15:47,131 Epoch 3:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:01:12     Step: 601/14376\n",
      "2025-06-26 10:15:58,861 Epoch 3:     Train MAE: 13.02     Val MAE: 23.54     Time: 0:01:24     Step: 701/14376\n",
      "2025-06-26 10:16:10,647 Epoch 3:     Train MAE: 13.60     Val MAE: 23.54     Time: 0:01:35     Step: 801/14376\n",
      "2025-06-26 10:16:22,367 Epoch 3:     Train MAE: 13.76     Val MAE: 23.54     Time: 0:01:47     Step: 901/14376\n",
      "2025-06-26 10:16:34,067 Epoch 3:     Train MAE: 14.08     Val MAE: 23.54     Time: 0:01:59     Step: 1001/14376\n",
      "2025-06-26 10:16:45,720 Epoch 3:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:02:10     Step: 1101/14376\n",
      "2025-06-26 10:16:57,603 Epoch 3:     Train MAE: 13.54     Val MAE: 23.54     Time: 0:02:22     Step: 1201/14376\n",
      "2025-06-26 10:17:09,756 Epoch 3:     Train MAE: 13.37     Val MAE: 23.54     Time: 0:02:34     Step: 1301/14376\n",
      "2025-06-26 10:17:21,967 Epoch 3:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:02:47     Step: 1401/14376\n",
      "2025-06-26 10:17:33,660 Epoch 3:     Train MAE: 13.14     Val MAE: 23.54     Time: 0:02:58     Step: 1501/14376\n",
      "2025-06-26 10:17:45,709 Epoch 3:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:03:10     Step: 1601/14376\n",
      "2025-06-26 10:17:57,386 Epoch 3:     Train MAE: 13.89     Val MAE: 23.54     Time: 0:03:22     Step: 1701/14376\n",
      "2025-06-26 10:18:09,086 Epoch 3:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:03:34     Step: 1801/14376\n",
      "2025-06-26 10:18:20,748 Epoch 3:     Train MAE: 13.21     Val MAE: 23.54     Time: 0:03:45     Step: 1901/14376\n",
      "2025-06-26 10:18:32,437 Epoch 3:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:03:57     Step: 2001/14376\n",
      "2025-06-26 10:18:44,126 Epoch 3:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:04:09     Step: 2101/14376\n",
      "2025-06-26 10:18:55,788 Epoch 3:     Train MAE: 13.33     Val MAE: 23.54     Time: 0:04:21     Step: 2201/14376\n",
      "2025-06-26 10:19:08,319 Epoch 3:     Train MAE: 13.58     Val MAE: 23.54     Time: 0:04:33     Step: 2301/14376\n",
      "2025-06-26 10:19:19,998 Epoch 3:     Train MAE: 13.75     Val MAE: 23.54     Time: 0:04:45     Step: 2401/14376\n",
      "2025-06-26 10:19:31,757 Epoch 3:     Train MAE: 13.54     Val MAE: 23.54     Time: 0:04:56     Step: 2501/14376\n",
      "2025-06-26 10:19:43,458 Epoch 3:     Train MAE: 13.72     Val MAE: 23.54     Time: 0:05:08     Step: 2601/14376\n",
      "2025-06-26 10:19:55,161 Epoch 3:     Train MAE: 13.22     Val MAE: 23.54     Time: 0:05:20     Step: 2701/14376\n",
      "2025-06-26 10:20:06,850 Epoch 3:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:05:32     Step: 2801/14376\n",
      "2025-06-26 10:20:18,535 Epoch 3:     Train MAE: 13.69     Val MAE: 23.54     Time: 0:05:43     Step: 2901/14376\n",
      "2025-06-26 10:20:30,196 Epoch 3:     Train MAE: 13.32     Val MAE: 23.54     Time: 0:05:55     Step: 3001/14376\n",
      "2025-06-26 10:20:42,091 Epoch 3:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:06:07     Step: 3101/14376\n",
      "2025-06-26 10:20:54,094 Epoch 3:     Train MAE: 13.80     Val MAE: 23.54     Time: 0:06:19     Step: 3201/14376\n",
      "2025-06-26 10:21:06,536 Epoch 3:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:06:31     Step: 3301/14376\n",
      "2025-06-26 10:21:18,216 Epoch 3:     Train MAE: 13.97     Val MAE: 23.54     Time: 0:06:43     Step: 3401/14376\n",
      "2025-06-26 10:21:29,878 Epoch 3:     Train MAE: 13.12     Val MAE: 23.54     Time: 0:06:55     Step: 3501/14376\n",
      "2025-06-26 10:21:41,524 Epoch 3:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:07:06     Step: 3601/14376\n",
      "2025-06-26 10:21:53,181 Epoch 3:     Train MAE: 13.28     Val MAE: 23.54     Time: 0:07:18     Step: 3701/14376\n",
      "2025-06-26 10:22:04,993 Epoch 3:     Train MAE: 13.55     Val MAE: 23.54     Time: 0:07:30     Step: 3801/14376\n",
      "2025-06-26 10:22:16,655 Epoch 3:     Train MAE: 13.28     Val MAE: 23.54     Time: 0:07:41     Step: 3901/14376\n",
      "2025-06-26 10:22:28,321 Epoch 3:     Train MAE: 13.66     Val MAE: 23.54     Time: 0:07:53     Step: 4001/14376\n",
      "2025-06-26 10:22:39,985 Epoch 3:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:08:05     Step: 4101/14376\n",
      "2025-06-26 10:22:51,657 Epoch 3:     Train MAE: 13.18     Val MAE: 23.54     Time: 0:08:16     Step: 4201/14376\n",
      "2025-06-26 10:23:04,117 Epoch 3:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:08:29     Step: 4301/14376\n",
      "2025-06-26 10:23:15,787 Epoch 3:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:08:41     Step: 4401/14376\n",
      "2025-06-26 10:23:27,444 Epoch 3:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:08:52     Step: 4501/14376\n",
      "2025-06-26 10:23:39,091 Epoch 3:     Train MAE: 13.18     Val MAE: 23.54     Time: 0:09:04     Step: 4601/14376\n",
      "2025-06-26 10:23:50,749 Epoch 3:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:09:15     Step: 4701/14376\n",
      "2025-06-26 10:24:02,961 Epoch 3:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:09:28     Step: 4801/14376\n",
      "2025-06-26 10:24:14,611 Epoch 3:     Train MAE: 13.42     Val MAE: 23.54     Time: 0:09:39     Step: 4901/14376\n",
      "2025-06-26 10:24:26,367 Epoch 3:     Train MAE: 13.59     Val MAE: 23.54     Time: 0:09:51     Step: 5001/14376\n",
      "2025-06-26 10:24:38,002 Epoch 3:     Train MAE: 13.71     Val MAE: 23.54     Time: 0:10:03     Step: 5101/14376\n",
      "2025-06-26 10:24:50,020 Epoch 3:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:10:15     Step: 5201/14376\n",
      "2025-06-26 10:25:02,678 Epoch 3:     Train MAE: 13.46     Val MAE: 23.54     Time: 0:10:27     Step: 5301/14376\n",
      "2025-06-26 10:25:14,297 Epoch 3:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:10:39     Step: 5401/14376\n",
      "2025-06-26 10:25:25,909 Epoch 3:     Train MAE: 13.61     Val MAE: 23.54     Time: 0:10:51     Step: 5501/14376\n",
      "2025-06-26 10:25:37,518 Epoch 3:     Train MAE: 13.05     Val MAE: 23.54     Time: 0:11:02     Step: 5601/14376\n",
      "2025-06-26 10:25:49,139 Epoch 3:     Train MAE: 13.20     Val MAE: 23.54     Time: 0:11:14     Step: 5701/14376\n",
      "2025-06-26 10:26:00,779 Epoch 3:     Train MAE: 13.69     Val MAE: 23.54     Time: 0:11:26     Step: 5801/14376\n",
      "2025-06-26 10:26:12,433 Epoch 3:     Train MAE: 13.64     Val MAE: 23.54     Time: 0:11:37     Step: 5901/14376\n",
      "2025-06-26 10:26:24,083 Epoch 3:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:11:49     Step: 6001/14376\n",
      "2025-06-26 10:26:35,720 Epoch 3:     Train MAE: 13.27     Val MAE: 23.54     Time: 0:12:00     Step: 6101/14376\n",
      "2025-06-26 10:26:47,789 Epoch 3:     Train MAE: 13.06     Val MAE: 23.54     Time: 0:12:13     Step: 6201/14376\n",
      "2025-06-26 10:26:59,811 Epoch 3:     Train MAE: 13.41     Val MAE: 23.54     Time: 0:12:25     Step: 6301/14376\n",
      "2025-06-26 10:27:11,590 Epoch 3:     Train MAE: 13.45     Val MAE: 23.54     Time: 0:12:36     Step: 6401/14376\n",
      "2025-06-26 10:27:23,341 Epoch 3:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:12:48     Step: 6501/14376\n",
      "2025-06-26 10:27:35,344 Epoch 3:     Train MAE: 13.19     Val MAE: 23.54     Time: 0:13:00     Step: 6601/14376\n",
      "2025-06-26 10:27:47,264 Epoch 3:     Train MAE: 13.45     Val MAE: 23.54     Time: 0:13:12     Step: 6701/14376\n",
      "2025-06-26 10:27:59,044 Epoch 3:     Train MAE: 13.11     Val MAE: 23.54     Time: 0:13:24     Step: 6801/14376\n",
      "2025-06-26 10:28:10,791 Epoch 3:     Train MAE: 13.31     Val MAE: 23.54     Time: 0:13:36     Step: 6901/14376\n",
      "2025-06-26 10:28:22,518 Epoch 3:     Train MAE: 13.36     Val MAE: 23.54     Time: 0:13:47     Step: 7001/14376\n",
      "2025-06-26 10:28:34,195 Epoch 3:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:13:59     Step: 7101/14376\n",
      "2025-06-26 10:28:46,294 Epoch 3:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:14:11     Step: 7201/14376\n",
      "2025-06-26 10:28:58,825 Epoch 3:     Train MAE: 13.81     Val MAE: 23.54     Time: 0:14:24     Step: 7301/14376\n",
      "2025-06-26 10:29:10,477 Epoch 3:     Train MAE: 13.78     Val MAE: 23.54     Time: 0:14:35     Step: 7401/14376\n",
      "2025-06-26 10:29:22,132 Epoch 3:     Train MAE: 13.73     Val MAE: 23.54     Time: 0:14:47     Step: 7501/14376\n",
      "2025-06-26 10:29:33,785 Epoch 3:     Train MAE: 13.03     Val MAE: 23.54     Time: 0:14:59     Step: 7601/14376\n",
      "2025-06-26 10:29:45,441 Epoch 3:     Train MAE: 13.70     Val MAE: 23.54     Time: 0:15:10     Step: 7701/14376\n",
      "2025-06-26 10:29:57,103 Epoch 3:     Train MAE: 13.41     Val MAE: 23.54     Time: 0:15:22     Step: 7801/14376\n",
      "2025-06-26 10:30:08,759 Epoch 3:     Train MAE: 13.24     Val MAE: 23.54     Time: 0:15:33     Step: 7901/14376\n",
      "2025-06-26 10:30:20,440 Epoch 3:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:15:45     Step: 8001/14376\n",
      "2025-06-26 10:30:32,432 Epoch 3:     Train MAE: 13.54     Val MAE: 23.54     Time: 0:15:57     Step: 8101/14376\n",
      "2025-06-26 10:30:45,155 Epoch 3:     Train MAE: 13.21     Val MAE: 23.54     Time: 0:16:10     Step: 8201/14376\n",
      "2025-06-26 10:30:56,875 Epoch 3:     Train MAE: 13.33     Val MAE: 23.54     Time: 0:16:22     Step: 8301/14376\n",
      "2025-06-26 10:31:08,526 Epoch 3:     Train MAE: 13.12     Val MAE: 23.54     Time: 0:16:33     Step: 8401/14376\n",
      "2025-06-26 10:31:20,167 Epoch 3:     Train MAE: 13.42     Val MAE: 23.54     Time: 0:16:45     Step: 8501/14376\n",
      "2025-06-26 10:31:31,828 Epoch 3:     Train MAE: 13.76     Val MAE: 23.54     Time: 0:16:57     Step: 8601/14376\n",
      "2025-06-26 10:31:43,473 Epoch 3:     Train MAE: 13.35     Val MAE: 23.54     Time: 0:17:08     Step: 8701/14376\n",
      "2025-06-26 10:31:55,113 Epoch 3:     Train MAE: 13.10     Val MAE: 23.54     Time: 0:17:20     Step: 8801/14376\n",
      "2025-06-26 10:32:07,467 Epoch 3:     Train MAE: 13.31     Val MAE: 23.54     Time: 0:17:32     Step: 8901/14376\n",
      "2025-06-26 10:32:19,905 Epoch 3:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:17:45     Step: 9001/14376\n",
      "2025-06-26 10:32:31,573 Epoch 3:     Train MAE: 13.08     Val MAE: 23.54     Time: 0:17:56     Step: 9101/14376\n",
      "2025-06-26 10:32:44,059 Epoch 3:     Train MAE: 13.19     Val MAE: 23.54     Time: 0:18:09     Step: 9201/14376\n",
      "2025-06-26 10:32:55,682 Epoch 3:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:18:20     Step: 9301/14376\n",
      "2025-06-26 10:33:07,414 Epoch 3:     Train MAE: 13.70     Val MAE: 23.54     Time: 0:18:32     Step: 9401/14376\n",
      "2025-06-26 10:33:19,053 Epoch 3:     Train MAE: 13.45     Val MAE: 23.54     Time: 0:18:44     Step: 9501/14376\n",
      "2025-06-26 10:33:30,703 Epoch 3:     Train MAE: 13.27     Val MAE: 23.54     Time: 0:18:55     Step: 9601/14376\n",
      "2025-06-26 10:33:42,928 Epoch 3:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:19:08     Step: 9701/14376\n",
      "2025-06-26 10:33:54,562 Epoch 3:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:19:19     Step: 9801/14376\n",
      "2025-06-26 10:34:06,199 Epoch 3:     Train MAE: 14.16     Val MAE: 23.54     Time: 0:19:31     Step: 9901/14376\n",
      "2025-06-26 10:34:17,832 Epoch 3:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:19:43     Step: 10001/14376\n",
      "2025-06-26 10:34:29,480 Epoch 3:     Train MAE: 13.60     Val MAE: 23.54     Time: 0:19:54     Step: 10101/14376\n",
      "2025-06-26 10:34:41,966 Epoch 3:     Train MAE: 13.20     Val MAE: 23.54     Time: 0:20:07     Step: 10201/14376\n",
      "2025-06-26 10:34:53,632 Epoch 3:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:20:18     Step: 10301/14376\n",
      "2025-06-26 10:35:05,263 Epoch 3:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:20:30     Step: 10401/14376\n",
      "2025-06-26 10:35:16,891 Epoch 3:     Train MAE: 13.87     Val MAE: 23.54     Time: 0:20:42     Step: 10501/14376\n",
      "2025-06-26 10:35:28,730 Epoch 3:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:20:53     Step: 10601/14376\n",
      "2025-06-26 10:35:40,379 Epoch 3:     Train MAE: 13.13     Val MAE: 23.54     Time: 0:21:05     Step: 10701/14376\n",
      "2025-06-26 10:35:52,042 Epoch 3:     Train MAE: 13.92     Val MAE: 23.54     Time: 0:21:17     Step: 10801/14376\n",
      "2025-06-26 10:36:03,681 Epoch 3:     Train MAE: 13.36     Val MAE: 23.54     Time: 0:21:28     Step: 10901/14376\n",
      "2025-06-26 10:36:15,318 Epoch 3:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:21:40     Step: 11001/14376\n",
      "2025-06-26 10:36:27,401 Epoch 3:     Train MAE: 13.19     Val MAE: 23.54     Time: 0:21:52     Step: 11101/14376\n",
      "2025-06-26 10:36:39,470 Epoch 3:     Train MAE: 13.28     Val MAE: 23.54     Time: 0:22:04     Step: 11201/14376\n",
      "2025-06-26 10:36:51,173 Epoch 3:     Train MAE: 13.24     Val MAE: 23.54     Time: 0:22:16     Step: 11301/14376\n",
      "2025-06-26 10:37:02,802 Epoch 3:     Train MAE: 13.59     Val MAE: 23.54     Time: 0:22:28     Step: 11401/14376\n",
      "2025-06-26 10:37:14,415 Epoch 3:     Train MAE: 13.87     Val MAE: 23.54     Time: 0:22:39     Step: 11501/14376\n",
      "2025-06-26 10:37:26,056 Epoch 3:     Train MAE: 13.88     Val MAE: 23.54     Time: 0:22:51     Step: 11601/14376\n",
      "2025-06-26 10:37:37,697 Epoch 3:     Train MAE: 13.20     Val MAE: 23.54     Time: 0:23:02     Step: 11701/14376\n",
      "2025-06-26 10:37:49,333 Epoch 3:     Train MAE: 13.90     Val MAE: 23.54     Time: 0:23:14     Step: 11801/14376\n",
      "2025-06-26 10:38:00,968 Epoch 3:     Train MAE: 13.41     Val MAE: 23.54     Time: 0:23:26     Step: 11901/14376\n",
      "2025-06-26 10:38:12,594 Epoch 3:     Train MAE: 13.23     Val MAE: 23.54     Time: 0:23:37     Step: 12001/14376\n",
      "2025-06-26 10:38:24,589 Epoch 3:     Train MAE: 13.69     Val MAE: 23.54     Time: 0:23:49     Step: 12101/14376\n",
      "2025-06-26 10:38:36,605 Epoch 3:     Train MAE: 13.18     Val MAE: 23.54     Time: 0:24:01     Step: 12201/14376\n",
      "2025-06-26 10:38:48,244 Epoch 3:     Train MAE: 13.88     Val MAE: 23.54     Time: 0:24:13     Step: 12301/14376\n",
      "2025-06-26 10:38:59,882 Epoch 3:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:24:25     Step: 12401/14376\n",
      "2025-06-26 10:39:11,507 Epoch 3:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:24:36     Step: 12501/14376\n",
      "2025-06-26 10:39:23,173 Epoch 3:     Train MAE: 13.79     Val MAE: 23.54     Time: 0:24:48     Step: 12601/14376\n",
      "2025-06-26 10:39:34,795 Epoch 3:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:25:00     Step: 12701/14376\n",
      "2025-06-26 10:39:46,414 Epoch 3:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:25:11     Step: 12801/14376\n",
      "2025-06-26 10:39:58,031 Epoch 3:     Train MAE: 13.14     Val MAE: 23.54     Time: 0:25:23     Step: 12901/14376\n",
      "2025-06-26 10:40:09,647 Epoch 3:     Train MAE: 13.18     Val MAE: 23.54     Time: 0:25:34     Step: 13001/14376\n",
      "2025-06-26 10:40:21,667 Epoch 3:     Train MAE: 13.76     Val MAE: 23.54     Time: 0:25:46     Step: 13101/14376\n",
      "2025-06-26 10:40:34,275 Epoch 3:     Train MAE: 13.77     Val MAE: 23.54     Time: 0:25:59     Step: 13201/14376\n",
      "2025-06-26 10:40:45,912 Epoch 3:     Train MAE: 13.31     Val MAE: 23.54     Time: 0:26:11     Step: 13301/14376\n",
      "2025-06-26 10:40:57,568 Epoch 3:     Train MAE: 13.40     Val MAE: 23.54     Time: 0:26:22     Step: 13401/14376\n",
      "2025-06-26 10:41:09,815 Epoch 3:     Train MAE: 12.83     Val MAE: 23.54     Time: 0:26:35     Step: 13501/14376\n",
      "2025-06-26 10:41:21,439 Epoch 3:     Train MAE: 13.26     Val MAE: 23.54     Time: 0:26:46     Step: 13601/14376\n",
      "2025-06-26 10:41:33,124 Epoch 3:     Train MAE: 13.11     Val MAE: 23.54     Time: 0:26:58     Step: 13701/14376\n",
      "2025-06-26 10:41:44,798 Epoch 3:     Train MAE: 13.52     Val MAE: 23.54     Time: 0:27:10     Step: 13801/14376\n",
      "2025-06-26 10:41:56,471 Epoch 3:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:27:21     Step: 13901/14376\n",
      "2025-06-26 10:42:08,153 Epoch 3:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:27:33     Step: 14001/14376\n",
      "2025-06-26 10:42:20,613 Epoch 3:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:27:45     Step: 14101/14376\n",
      "2025-06-26 10:42:32,283 Epoch 3:     Train MAE: 13.31     Val MAE: 23.54     Time: 0:27:57     Step: 14201/14376\n",
      "2025-06-26 10:42:43,957 Epoch 3:     Train MAE: 13.25     Val MAE: 23.54     Time: 0:28:09     Step: 14301/14376\n",
      "100%|█████████████████████████████████████████| 313/313 [00:09<00:00, 33.81it/s]\n",
      "2025-06-26 10:43:05,062 [Epoch 3] lr: 5.000000000e-06\n",
      "2025-06-26 10:43:05,062 New best: 23.54 -> 23.54\n",
      "2025-06-26 10:43:05,062 Saved weights..\n",
      "2025-06-26 10:43:05,859 Epoch 4:     Train MAE: 13.27     Val MAE: 23.54     Time: 0:00:00     Step: 1/14376\n",
      "2025-06-26 10:43:17,435 Epoch 4:     Train MAE: 13.58     Val MAE: 23.54     Time: 0:00:12     Step: 101/14376\n",
      "2025-06-26 10:43:29,023 Epoch 4:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:00:24     Step: 201/14376\n",
      "2025-06-26 10:43:40,620 Epoch 4:     Train MAE: 13.35     Val MAE: 23.54     Time: 0:00:35     Step: 301/14376\n",
      "2025-06-26 10:43:52,228 Epoch 4:     Train MAE: 13.31     Val MAE: 23.54     Time: 0:00:47     Step: 401/14376\n",
      "2025-06-26 10:44:03,849 Epoch 4:     Train MAE: 13.75     Val MAE: 23.54     Time: 0:00:58     Step: 501/14376\n",
      "2025-06-26 10:44:15,731 Epoch 4:     Train MAE: 12.74     Val MAE: 23.54     Time: 0:01:10     Step: 601/14376\n",
      "2025-06-26 10:44:28,385 Epoch 4:     Train MAE: 13.09     Val MAE: 23.54     Time: 0:01:23     Step: 701/14376\n",
      "2025-06-26 10:44:40,013 Epoch 4:     Train MAE: 13.37     Val MAE: 23.54     Time: 0:01:35     Step: 801/14376\n",
      "2025-06-26 10:44:51,642 Epoch 4:     Train MAE: 13.11     Val MAE: 23.54     Time: 0:01:46     Step: 901/14376\n",
      "2025-06-26 10:45:03,681 Epoch 4:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:01:58     Step: 1001/14376\n",
      "2025-06-26 10:45:15,379 Epoch 4:     Train MAE: 13.35     Val MAE: 23.54     Time: 0:02:10     Step: 1101/14376\n",
      "2025-06-26 10:45:27,101 Epoch 4:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:02:22     Step: 1201/14376\n",
      "2025-06-26 10:45:38,824 Epoch 4:     Train MAE: 13.14     Val MAE: 23.54     Time: 0:02:33     Step: 1301/14376\n",
      "2025-06-26 10:45:51,059 Epoch 4:     Train MAE: 13.19     Val MAE: 23.54     Time: 0:02:46     Step: 1401/14376\n",
      "2025-06-26 10:46:02,840 Epoch 4:     Train MAE: 13.95     Val MAE: 23.54     Time: 0:02:57     Step: 1501/14376\n",
      "2025-06-26 10:46:14,774 Epoch 4:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:03:09     Step: 1601/14376\n",
      "2025-06-26 10:46:27,625 Epoch 4:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:03:22     Step: 1701/14376\n",
      "2025-06-26 10:46:39,374 Epoch 4:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:03:34     Step: 1801/14376\n",
      "2025-06-26 10:46:51,000 Epoch 4:     Train MAE: 13.89     Val MAE: 23.54     Time: 0:03:46     Step: 1901/14376\n",
      "2025-06-26 10:47:02,627 Epoch 4:     Train MAE: 13.67     Val MAE: 23.54     Time: 0:03:57     Step: 2001/14376\n",
      "2025-06-26 10:47:14,512 Epoch 4:     Train MAE: 13.52     Val MAE: 23.54     Time: 0:04:09     Step: 2101/14376\n",
      "2025-06-26 10:47:26,514 Epoch 4:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:04:21     Step: 2201/14376\n",
      "2025-06-26 10:47:38,248 Epoch 4:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:04:33     Step: 2301/14376\n",
      "2025-06-26 10:47:49,952 Epoch 4:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:04:44     Step: 2401/14376\n",
      "2025-06-26 10:48:01,652 Epoch 4:     Train MAE: 13.20     Val MAE: 23.54     Time: 0:04:56     Step: 2501/14376\n",
      "2025-06-26 10:48:13,758 Epoch 4:     Train MAE: 13.22     Val MAE: 23.54     Time: 0:05:08     Step: 2601/14376\n",
      "2025-06-26 10:48:25,851 Epoch 4:     Train MAE: 13.11     Val MAE: 23.54     Time: 0:05:20     Step: 2701/14376\n",
      "2025-06-26 10:48:37,504 Epoch 4:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:05:32     Step: 2801/14376\n",
      "2025-06-26 10:48:49,217 Epoch 4:     Train MAE: 12.90     Val MAE: 23.54     Time: 0:05:44     Step: 2901/14376\n",
      "2025-06-26 10:49:00,970 Epoch 4:     Train MAE: 13.76     Val MAE: 23.54     Time: 0:05:55     Step: 3001/14376\n",
      "2025-06-26 10:49:12,682 Epoch 4:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:06:07     Step: 3101/14376\n",
      "2025-06-26 10:49:24,538 Epoch 4:     Train MAE: 13.93     Val MAE: 23.54     Time: 0:06:19     Step: 3201/14376\n",
      "2025-06-26 10:49:36,277 Epoch 4:     Train MAE: 13.69     Val MAE: 23.54     Time: 0:06:31     Step: 3301/14376\n",
      "2025-06-26 10:49:48,010 Epoch 4:     Train MAE: 13.64     Val MAE: 23.54     Time: 0:06:43     Step: 3401/14376\n",
      "2025-06-26 10:49:59,747 Epoch 4:     Train MAE: 13.44     Val MAE: 23.54     Time: 0:06:54     Step: 3501/14376\n",
      "2025-06-26 10:50:12,208 Epoch 4:     Train MAE: 13.95     Val MAE: 23.54     Time: 0:07:07     Step: 3601/14376\n",
      "2025-06-26 10:50:24,624 Epoch 4:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:07:19     Step: 3701/14376\n",
      "2025-06-26 10:50:36,326 Epoch 4:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:07:31     Step: 3801/14376\n",
      "2025-06-26 10:50:47,976 Epoch 4:     Train MAE: 13.21     Val MAE: 23.54     Time: 0:07:42     Step: 3901/14376\n",
      "2025-06-26 10:50:59,703 Epoch 4:     Train MAE: 12.82     Val MAE: 23.54     Time: 0:07:54     Step: 4001/14376\n",
      "2025-06-26 10:51:11,376 Epoch 4:     Train MAE: 13.46     Val MAE: 23.54     Time: 0:08:06     Step: 4101/14376\n",
      "2025-06-26 10:51:23,049 Epoch 4:     Train MAE: 13.16     Val MAE: 23.54     Time: 0:08:18     Step: 4201/14376\n",
      "2025-06-26 10:51:34,724 Epoch 4:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:08:29     Step: 4301/14376\n",
      "2025-06-26 10:51:46,639 Epoch 4:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:08:41     Step: 4401/14376\n",
      "2025-06-26 10:51:58,340 Epoch 4:     Train MAE: 13.67     Val MAE: 23.54     Time: 0:08:53     Step: 4501/14376\n",
      "2025-06-26 10:52:10,448 Epoch 4:     Train MAE: 13.70     Val MAE: 23.54     Time: 0:09:05     Step: 4601/14376\n",
      "2025-06-26 10:52:22,505 Epoch 4:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:09:17     Step: 4701/14376\n",
      "2025-06-26 10:52:34,257 Epoch 4:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:09:29     Step: 4801/14376\n",
      "2025-06-26 10:52:45,939 Epoch 4:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:09:40     Step: 4901/14376\n",
      "2025-06-26 10:52:57,622 Epoch 4:     Train MAE: 13.42     Val MAE: 23.54     Time: 0:09:52     Step: 5001/14376\n",
      "2025-06-26 10:53:09,318 Epoch 4:     Train MAE: 13.03     Val MAE: 23.54     Time: 0:10:04     Step: 5101/14376\n",
      "2025-06-26 10:53:21,109 Epoch 4:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:10:16     Step: 5201/14376\n",
      "2025-06-26 10:53:32,878 Epoch 4:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:10:27     Step: 5301/14376\n",
      "2025-06-26 10:53:44,618 Epoch 4:     Train MAE: 13.57     Val MAE: 23.54     Time: 0:10:39     Step: 5401/14376\n",
      "2025-06-26 10:53:56,268 Epoch 4:     Train MAE: 13.19     Val MAE: 23.54     Time: 0:10:51     Step: 5501/14376\n",
      "2025-06-26 10:54:08,719 Epoch 4:     Train MAE: 13.04     Val MAE: 23.54     Time: 0:11:03     Step: 5601/14376\n",
      "2025-06-26 10:54:21,084 Epoch 4:     Train MAE: 13.22     Val MAE: 23.54     Time: 0:11:16     Step: 5701/14376\n",
      "2025-06-26 10:54:32,762 Epoch 4:     Train MAE: 13.63     Val MAE: 23.54     Time: 0:11:27     Step: 5801/14376\n",
      "2025-06-26 10:54:44,397 Epoch 4:     Train MAE: 13.80     Val MAE: 23.54     Time: 0:11:39     Step: 5901/14376\n",
      "2025-06-26 10:54:56,119 Epoch 4:     Train MAE: 13.39     Val MAE: 23.54     Time: 0:11:51     Step: 6001/14376\n",
      "2025-06-26 10:55:07,771 Epoch 4:     Train MAE: 13.68     Val MAE: 23.54     Time: 0:12:02     Step: 6101/14376\n",
      "2025-06-26 10:55:19,432 Epoch 4:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:12:14     Step: 6201/14376\n",
      "2025-06-26 10:55:31,836 Epoch 4:     Train MAE: 13.44     Val MAE: 23.54     Time: 0:12:26     Step: 6301/14376\n",
      "2025-06-26 10:55:43,549 Epoch 4:     Train MAE: 13.54     Val MAE: 23.54     Time: 0:12:38     Step: 6401/14376\n",
      "2025-06-26 10:55:55,717 Epoch 4:     Train MAE: 13.76     Val MAE: 23.54     Time: 0:12:50     Step: 6501/14376\n",
      "2025-06-26 10:56:08,257 Epoch 4:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:13:03     Step: 6601/14376\n",
      "2025-06-26 10:56:20,079 Epoch 4:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:13:15     Step: 6701/14376\n",
      "2025-06-26 10:56:32,066 Epoch 4:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:13:27     Step: 6801/14376\n",
      "2025-06-26 10:56:44,258 Epoch 4:     Train MAE: 13.66     Val MAE: 23.54     Time: 0:13:39     Step: 6901/14376\n",
      "2025-06-26 10:56:56,502 Epoch 4:     Train MAE: 13.24     Val MAE: 23.54     Time: 0:13:51     Step: 7001/14376\n",
      "2025-06-26 10:57:08,177 Epoch 4:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:14:03     Step: 7101/14376\n",
      "2025-06-26 10:57:19,822 Epoch 4:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:14:14     Step: 7201/14376\n",
      "2025-06-26 10:57:31,529 Epoch 4:     Train MAE: 13.32     Val MAE: 23.54     Time: 0:14:26     Step: 7301/14376\n",
      "2025-06-26 10:57:43,433 Epoch 4:     Train MAE: 13.62     Val MAE: 23.54     Time: 0:14:38     Step: 7401/14376\n",
      "2025-06-26 10:57:55,812 Epoch 4:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:14:50     Step: 7501/14376\n",
      "2025-06-26 10:58:07,862 Epoch 4:     Train MAE: 13.24     Val MAE: 23.54     Time: 0:15:02     Step: 7601/14376\n",
      "2025-06-26 10:58:19,527 Epoch 4:     Train MAE: 13.83     Val MAE: 23.54     Time: 0:15:14     Step: 7701/14376\n",
      "2025-06-26 10:58:31,203 Epoch 4:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:15:26     Step: 7801/14376\n",
      "2025-06-26 10:58:42,895 Epoch 4:     Train MAE: 13.73     Val MAE: 23.54     Time: 0:15:37     Step: 7901/14376\n",
      "2025-06-26 10:58:54,566 Epoch 4:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:15:49     Step: 8001/14376\n",
      "2025-06-26 10:59:06,233 Epoch 4:     Train MAE: 13.15     Val MAE: 23.54     Time: 0:16:01     Step: 8101/14376\n",
      "2025-06-26 10:59:17,918 Epoch 4:     Train MAE: 13.24     Val MAE: 23.54     Time: 0:16:12     Step: 8201/14376\n",
      "2025-06-26 10:59:29,598 Epoch 4:     Train MAE: 13.60     Val MAE: 23.54     Time: 0:16:24     Step: 8301/14376\n",
      "2025-06-26 10:59:41,324 Epoch 4:     Train MAE: 13.32     Val MAE: 23.54     Time: 0:16:36     Step: 8401/14376\n",
      "2025-06-26 10:59:53,491 Epoch 4:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:16:48     Step: 8501/14376\n",
      "2025-06-26 11:00:05,567 Epoch 4:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:17:00     Step: 8601/14376\n",
      "2025-06-26 11:00:17,264 Epoch 4:     Train MAE: 13.84     Val MAE: 23.54     Time: 0:17:12     Step: 8701/14376\n",
      "2025-06-26 11:00:28,908 Epoch 4:     Train MAE: 13.19     Val MAE: 23.54     Time: 0:17:23     Step: 8801/14376\n",
      "2025-06-26 11:00:40,561 Epoch 4:     Train MAE: 13.26     Val MAE: 23.54     Time: 0:17:35     Step: 8901/14376\n",
      "2025-06-26 11:00:52,238 Epoch 4:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:17:47     Step: 9001/14376\n",
      "2025-06-26 11:01:03,916 Epoch 4:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:17:58     Step: 9101/14376\n",
      "2025-06-26 11:01:15,897 Epoch 4:     Train MAE: 13.65     Val MAE: 23.54     Time: 0:18:10     Step: 9201/14376\n",
      "2025-06-26 11:01:27,715 Epoch 4:     Train MAE: 13.61     Val MAE: 23.54     Time: 0:18:22     Step: 9301/14376\n",
      "2025-06-26 11:01:39,396 Epoch 4:     Train MAE: 13.37     Val MAE: 23.54     Time: 0:18:34     Step: 9401/14376\n",
      "2025-06-26 11:01:51,545 Epoch 4:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:18:46     Step: 9501/14376\n",
      "2025-06-26 11:02:03,645 Epoch 4:     Train MAE: 13.41     Val MAE: 23.54     Time: 0:18:58     Step: 9601/14376\n",
      "2025-06-26 11:02:15,662 Epoch 4:     Train MAE: 13.09     Val MAE: 23.54     Time: 0:19:10     Step: 9701/14376\n",
      "2025-06-26 11:02:27,434 Epoch 4:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:19:22     Step: 9801/14376\n",
      "2025-06-26 11:02:39,200 Epoch 4:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:19:34     Step: 9901/14376\n",
      "2025-06-26 11:02:50,941 Epoch 4:     Train MAE: 13.38     Val MAE: 23.54     Time: 0:19:45     Step: 10001/14376\n",
      "2025-06-26 11:03:02,999 Epoch 4:     Train MAE: 13.50     Val MAE: 23.54     Time: 0:19:58     Step: 10101/14376\n",
      "2025-06-26 11:03:15,147 Epoch 4:     Train MAE: 13.26     Val MAE: 23.54     Time: 0:20:10     Step: 10201/14376\n",
      "2025-06-26 11:03:26,875 Epoch 4:     Train MAE: 13.52     Val MAE: 23.54     Time: 0:20:21     Step: 10301/14376\n",
      "2025-06-26 11:03:38,811 Epoch 4:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:20:33     Step: 10401/14376\n",
      "2025-06-26 11:03:50,937 Epoch 4:     Train MAE: 13.37     Val MAE: 23.54     Time: 0:20:45     Step: 10501/14376\n",
      "2025-06-26 11:04:03,528 Epoch 4:     Train MAE: 13.53     Val MAE: 23.54     Time: 0:20:58     Step: 10601/14376\n",
      "2025-06-26 11:04:15,578 Epoch 4:     Train MAE: 13.31     Val MAE: 23.54     Time: 0:21:10     Step: 10701/14376\n",
      "2025-06-26 11:04:27,249 Epoch 4:     Train MAE: 13.27     Val MAE: 23.54     Time: 0:21:22     Step: 10801/14376\n",
      "2025-06-26 11:04:39,362 Epoch 4:     Train MAE: 13.40     Val MAE: 23.54     Time: 0:21:34     Step: 10901/14376\n",
      "2025-06-26 11:04:51,052 Epoch 4:     Train MAE: 13.26     Val MAE: 23.54     Time: 0:21:46     Step: 11001/14376\n",
      "2025-06-26 11:05:02,724 Epoch 4:     Train MAE: 13.03     Val MAE: 23.54     Time: 0:21:57     Step: 11101/14376\n",
      "2025-06-26 11:05:14,697 Epoch 4:     Train MAE: 14.02     Val MAE: 23.54     Time: 0:22:09     Step: 11201/14376\n",
      "2025-06-26 11:05:26,675 Epoch 4:     Train MAE: 13.47     Val MAE: 23.54     Time: 0:22:21     Step: 11301/14376\n",
      "2025-06-26 11:05:38,386 Epoch 4:     Train MAE: 13.44     Val MAE: 23.54     Time: 0:22:33     Step: 11401/14376\n",
      "2025-06-26 11:05:50,870 Epoch 4:     Train MAE: 12.85     Val MAE: 23.54     Time: 0:22:45     Step: 11501/14376\n",
      "2025-06-26 11:06:02,798 Epoch 4:     Train MAE: 13.29     Val MAE: 23.54     Time: 0:22:57     Step: 11601/14376\n",
      "2025-06-26 11:06:14,462 Epoch 4:     Train MAE: 13.79     Val MAE: 23.54     Time: 0:23:09     Step: 11701/14376\n",
      "2025-06-26 11:06:26,223 Epoch 4:     Train MAE: 13.58     Val MAE: 23.54     Time: 0:23:21     Step: 11801/14376\n",
      "2025-06-26 11:06:37,865 Epoch 4:     Train MAE: 13.34     Val MAE: 23.54     Time: 0:23:32     Step: 11901/14376\n",
      "2025-06-26 11:06:49,563 Epoch 4:     Train MAE: 13.36     Val MAE: 23.54     Time: 0:23:44     Step: 12001/14376\n",
      "2025-06-26 11:07:01,226 Epoch 4:     Train MAE: 13.44     Val MAE: 23.54     Time: 0:23:56     Step: 12101/14376\n",
      "2025-06-26 11:07:12,962 Epoch 4:     Train MAE: 13.42     Val MAE: 23.54     Time: 0:24:07     Step: 12201/14376\n",
      "2025-06-26 11:07:25,161 Epoch 4:     Train MAE: 13.33     Val MAE: 23.54     Time: 0:24:20     Step: 12301/14376\n",
      "2025-06-26 11:07:36,940 Epoch 4:     Train MAE: 13.51     Val MAE: 23.54     Time: 0:24:31     Step: 12401/14376\n",
      "2025-06-26 11:07:49,424 Epoch 4:     Train MAE: 13.48     Val MAE: 23.54     Time: 0:24:44     Step: 12501/14376\n",
      "2025-06-26 11:08:01,260 Epoch 4:     Train MAE: 12.96     Val MAE: 23.54     Time: 0:24:56     Step: 12601/14376\n",
      "2025-06-26 11:08:12,898 Epoch 4:     Train MAE: 13.28     Val MAE: 23.54     Time: 0:25:07     Step: 12701/14376\n",
      "2025-06-26 11:08:24,750 Epoch 4:     Train MAE: 13.02     Val MAE: 23.54     Time: 0:25:19     Step: 12801/14376\n",
      "2025-06-26 11:08:36,400 Epoch 4:     Train MAE: 13.43     Val MAE: 23.54     Time: 0:25:31     Step: 12901/14376\n",
      "2025-06-26 11:08:48,092 Epoch 4:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:25:43     Step: 13001/14376\n",
      "2025-06-26 11:08:59,797 Epoch 4:     Train MAE: 13.11     Val MAE: 23.54     Time: 0:25:54     Step: 13101/14376\n",
      "2025-06-26 11:09:11,578 Epoch 4:     Train MAE: 13.35     Val MAE: 23.54     Time: 0:26:06     Step: 13201/14376\n",
      "2025-06-26 11:09:23,216 Epoch 4:     Train MAE: 13.42     Val MAE: 23.54     Time: 0:26:18     Step: 13301/14376\n",
      "2025-06-26 11:09:35,322 Epoch 4:     Train MAE: 13.59     Val MAE: 23.54     Time: 0:26:30     Step: 13401/14376\n",
      "2025-06-26 11:09:47,313 Epoch 4:     Train MAE: 13.75     Val MAE: 23.54     Time: 0:26:42     Step: 13501/14376\n",
      "2025-06-26 11:09:59,039 Epoch 4:     Train MAE: 13.56     Val MAE: 23.54     Time: 0:26:54     Step: 13601/14376\n",
      "2025-06-26 11:10:10,670 Epoch 4:     Train MAE: 13.30     Val MAE: 23.54     Time: 0:27:05     Step: 13701/14376\n",
      "2025-06-26 11:10:22,286 Epoch 4:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:27:17     Step: 13801/14376\n",
      "2025-06-26 11:10:33,897 Epoch 4:     Train MAE: 13.49     Val MAE: 23.54     Time: 0:27:28     Step: 13901/14376\n",
      "2025-06-26 11:10:45,661 Epoch 4:     Train MAE: 13.10     Val MAE: 23.54     Time: 0:27:40     Step: 14001/14376\n",
      "2025-06-26 11:10:57,298 Epoch 4:     Train MAE: 13.12     Val MAE: 23.54     Time: 0:27:52     Step: 14101/14376\n",
      "2025-06-26 11:11:08,988 Epoch 4:     Train MAE: 13.79     Val MAE: 23.54     Time: 0:28:04     Step: 14201/14376\n",
      "2025-06-26 11:11:20,646 Epoch 4:     Train MAE: 13.05     Val MAE: 23.54     Time: 0:28:15     Step: 14301/14376\n",
      "100%|█████████████████████████████████████████| 313/313 [00:09<00:00, 33.49it/s]\n",
      "2025-06-26 11:11:41,702 [Epoch 4] lr: 5.000000000e-06\n",
      "2025-06-26 11:11:42,121 Epoch 5:     Train MAE: 16.32     Val MAE: 23.56     Time: 0:00:00     Step: 1/14376\n",
      "2025-06-26 11:11:54,460 Epoch 5:     Train MAE: 13.74     Val MAE: 23.56     Time: 0:00:13     Step: 101/14376\n",
      "2025-06-26 11:12:06,077 Epoch 5:     Train MAE: 13.26     Val MAE: 23.56     Time: 0:00:24     Step: 201/14376\n",
      "2025-06-26 11:12:17,712 Epoch 5:     Train MAE: 13.16     Val MAE: 23.56     Time: 0:00:36     Step: 301/14376\n",
      "2025-06-26 11:12:29,336 Epoch 5:     Train MAE: 13.89     Val MAE: 23.56     Time: 0:00:48     Step: 401/14376\n",
      "2025-06-26 11:12:40,956 Epoch 5:     Train MAE: 13.42     Val MAE: 23.56     Time: 0:00:59     Step: 501/14376\n",
      "2025-06-26 11:12:52,574 Epoch 5:     Train MAE: 13.25     Val MAE: 23.56     Time: 0:01:11     Step: 601/14376\n",
      "2025-06-26 11:13:04,201 Epoch 5:     Train MAE: 13.23     Val MAE: 23.56     Time: 0:01:22     Step: 701/14376\n",
      "2025-06-26 11:13:15,835 Epoch 5:     Train MAE: 13.56     Val MAE: 23.56     Time: 0:01:34     Step: 801/14376\n",
      "2025-06-26 11:13:27,487 Epoch 5:     Train MAE: 13.31     Val MAE: 23.56     Time: 0:01:46     Step: 901/14376\n",
      "2025-06-26 11:13:39,587 Epoch 5:     Train MAE: 13.26     Val MAE: 23.56     Time: 0:01:58     Step: 1001/14376\n",
      "2025-06-26 11:13:51,669 Epoch 5:     Train MAE: 13.58     Val MAE: 23.56     Time: 0:02:10     Step: 1101/14376\n",
      "2025-06-26 11:14:03,331 Epoch 5:     Train MAE: 13.04     Val MAE: 23.56     Time: 0:02:22     Step: 1201/14376\n",
      "2025-06-26 11:14:15,033 Epoch 5:     Train MAE: 13.47     Val MAE: 23.56     Time: 0:02:33     Step: 1301/14376\n",
      "2025-06-26 11:14:26,683 Epoch 5:     Train MAE: 13.28     Val MAE: 23.56     Time: 0:02:45     Step: 1401/14376\n",
      "2025-06-26 11:14:38,319 Epoch 5:     Train MAE: 13.30     Val MAE: 23.56     Time: 0:02:57     Step: 1501/14376\n",
      "2025-06-26 11:14:49,954 Epoch 5:     Train MAE: 13.37     Val MAE: 23.56     Time: 0:03:08     Step: 1601/14376\n",
      "2025-06-26 11:15:01,925 Epoch 5:     Train MAE: 13.22     Val MAE: 23.56     Time: 0:03:20     Step: 1701/14376\n",
      "2025-06-26 11:15:13,551 Epoch 5:     Train MAE: 13.44     Val MAE: 23.56     Time: 0:03:32     Step: 1801/14376\n",
      "2025-06-26 11:15:25,214 Epoch 5:     Train MAE: 13.70     Val MAE: 23.56     Time: 0:03:44     Step: 1901/14376\n",
      "2025-06-26 11:15:37,265 Epoch 5:     Train MAE: 13.63     Val MAE: 23.56     Time: 0:03:56     Step: 2001/14376\n",
      "2025-06-26 11:15:49,271 Epoch 5:     Train MAE: 13.12     Val MAE: 23.56     Time: 0:04:08     Step: 2101/14376\n",
      "2025-06-26 11:16:00,911 Epoch 5:     Train MAE: 13.32     Val MAE: 23.56     Time: 0:04:19     Step: 2201/14376\n",
      "2025-06-26 11:16:12,557 Epoch 5:     Train MAE: 13.55     Val MAE: 23.56     Time: 0:04:31     Step: 2301/14376\n",
      "2025-06-26 11:16:24,189 Epoch 5:     Train MAE: 13.17     Val MAE: 23.56     Time: 0:04:42     Step: 2401/14376\n",
      "2025-06-26 11:16:35,816 Epoch 5:     Train MAE: 13.26     Val MAE: 23.56     Time: 0:04:54     Step: 2501/14376\n",
      "2025-06-26 11:16:47,447 Epoch 5:     Train MAE: 13.31     Val MAE: 23.56     Time: 0:05:06     Step: 2601/14376\n",
      "2025-06-26 11:16:59,531 Epoch 5:     Train MAE: 13.21     Val MAE: 23.56     Time: 0:05:18     Step: 2701/14376\n",
      "2025-06-26 11:17:11,165 Epoch 5:     Train MAE: 13.29     Val MAE: 23.56     Time: 0:05:29     Step: 2801/14376\n",
      "2025-06-26 11:17:22,799 Epoch 5:     Train MAE: 13.50     Val MAE: 23.56     Time: 0:05:41     Step: 2901/14376\n",
      "2025-06-26 11:17:34,802 Epoch 5:     Train MAE: 13.50     Val MAE: 23.56     Time: 0:05:53     Step: 3001/14376\n",
      "2025-06-26 11:17:46,851 Epoch 5:     Train MAE: 14.07     Val MAE: 23.56     Time: 0:06:05     Step: 3101/14376\n",
      "2025-06-26 11:17:58,473 Epoch 5:     Train MAE: 13.11     Val MAE: 23.56     Time: 0:06:17     Step: 3201/14376\n",
      "2025-06-26 11:18:10,109 Epoch 5:     Train MAE: 13.47     Val MAE: 23.56     Time: 0:06:28     Step: 3301/14376\n",
      "2025-06-26 11:18:21,755 Epoch 5:     Train MAE: 13.55     Val MAE: 23.56     Time: 0:06:40     Step: 3401/14376\n",
      "2025-06-26 11:18:33,376 Epoch 5:     Train MAE: 13.25     Val MAE: 23.56     Time: 0:06:52     Step: 3501/14376\n",
      "2025-06-26 11:18:45,016 Epoch 5:     Train MAE: 13.40     Val MAE: 23.56     Time: 0:07:03     Step: 3601/14376\n",
      "2025-06-26 11:18:56,656 Epoch 5:     Train MAE: 13.48     Val MAE: 23.56     Time: 0:07:15     Step: 3701/14376\n",
      "2025-06-26 11:19:08,347 Epoch 5:     Train MAE: 13.16     Val MAE: 23.56     Time: 0:07:27     Step: 3801/14376\n",
      "2025-06-26 11:19:20,334 Epoch 5:     Train MAE: 13.55     Val MAE: 23.56     Time: 0:07:39     Step: 3901/14376\n",
      "2025-06-26 11:19:32,624 Epoch 5:     Train MAE: 13.58     Val MAE: 23.56     Time: 0:07:51     Step: 4001/14376\n",
      "2025-06-26 11:19:45,134 Epoch 5:     Train MAE: 13.38     Val MAE: 23.56     Time: 0:08:03     Step: 4101/14376\n",
      "2025-06-26 11:19:57,562 Epoch 5:     Train MAE: 13.06     Val MAE: 23.56     Time: 0:08:16     Step: 4201/14376\n",
      "2025-06-26 11:20:09,219 Epoch 5:     Train MAE: 13.89     Val MAE: 23.56     Time: 0:08:28     Step: 4301/14376\n",
      "2025-06-26 11:20:20,882 Epoch 5:     Train MAE: 13.16     Val MAE: 23.56     Time: 0:08:39     Step: 4401/14376\n",
      "2025-06-26 11:20:32,546 Epoch 5:     Train MAE: 13.54     Val MAE: 23.56     Time: 0:08:51     Step: 4501/14376\n",
      "2025-06-26 11:20:44,251 Epoch 5:     Train MAE: 13.49     Val MAE: 23.56     Time: 0:09:03     Step: 4601/14376\n",
      "2025-06-26 11:20:55,902 Epoch 5:     Train MAE: 12.84     Val MAE: 23.56     Time: 0:09:14     Step: 4701/14376\n",
      "2025-06-26 11:21:07,567 Epoch 5:     Train MAE: 13.42     Val MAE: 23.56     Time: 0:09:26     Step: 4801/14376\n",
      "2025-06-26 11:21:19,658 Epoch 5:     Train MAE: 13.41     Val MAE: 23.56     Time: 0:09:38     Step: 4901/14376\n",
      "2025-06-26 11:21:31,739 Epoch 5:     Train MAE: 13.43     Val MAE: 23.56     Time: 0:09:50     Step: 5001/14376\n",
      "2025-06-26 11:21:43,740 Epoch 5:     Train MAE: 13.36     Val MAE: 23.56     Time: 0:10:02     Step: 5101/14376\n",
      "2025-06-26 11:21:55,448 Epoch 5:     Train MAE: 13.37     Val MAE: 23.56     Time: 0:10:14     Step: 5201/14376\n",
      "2025-06-26 11:22:07,118 Epoch 5:     Train MAE: 12.87     Val MAE: 23.56     Time: 0:10:25     Step: 5301/14376\n",
      "2025-06-26 11:22:18,796 Epoch 5:     Train MAE: 13.65     Val MAE: 23.56     Time: 0:10:37     Step: 5401/14376\n",
      "2025-06-26 11:22:30,450 Epoch 5:     Train MAE: 13.21     Val MAE: 23.56     Time: 0:10:49     Step: 5501/14376\n",
      "2025-06-26 11:22:42,125 Epoch 5:     Train MAE: 13.52     Val MAE: 23.56     Time: 0:11:00     Step: 5601/14376\n",
      "2025-06-26 11:22:53,893 Epoch 5:     Train MAE: 13.26     Val MAE: 23.56     Time: 0:11:12     Step: 5701/14376\n",
      "2025-06-26 11:23:05,576 Epoch 5:     Train MAE: 13.41     Val MAE: 23.56     Time: 0:11:24     Step: 5801/14376\n"
     ]
    }
   ],
   "source": [
    "if RUN_TRAIN:\n",
    "    print(\"Starting training..\")\n",
    "    !OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 _train.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11756775,
     "sourceId": 39763,
     "sourceType": "competition"
    },
    {
     "datasetId": 7253205,
     "sourceId": 11568812,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7253605,
     "sourceId": 11569667,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7253661,
     "sourceId": 11569755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7377931,
     "sourceId": 11887338,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
